
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 9 &#8212; Philstats</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Chap09';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Homework 2" href="Homework02.html" />
    <link rel="prev" title="Chapter 8" href="Chap08.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/philstats.png" class="logo__image only-light" alt="Philstats - Home"/>
    <script>document.write(`<img src="_static/philstats.png" class="logo__image only-dark" alt="Philstats - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Philstats
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Chap01.html">Chapter 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chap02.html">Chapter 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chap03.html">Chapter 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chap04.html">Chapter 4</a></li>
<li class="toctree-l1"><a class="reference internal" href="Homework01.html">Homework 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chap05.html">Chapter 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chap06.html">Chapter 6</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chap07.html">Chapter 7</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chap08.html">Chapter 8</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 9</a></li>



<li class="toctree-l1"><a class="reference internal" href="Homework02.html">Homework 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/logic-teaching/philstatsbook/main?urlpath=lab/tree/Chap09.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Chap09.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 9</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 9</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-and-the-task-of-statistical-inference">Bayes’ Theorem and the task of statistical inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-task-of-statistical-inference">The task of statistical inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vivifying-bayes-theorem">Vivifying Bayes’ Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-components-of-bayes-theorem">The components of Bayes’ Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advice-on-keeping-the-notation-straight">Advice on keeping the notation straight:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-example-uniform-vs-three-normals-plus-prior-which-is-uniform">A simple example: uniform vs. three normals, plus prior which is uniform</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-a-one-step-application-of-bayes-theorem-the-prominence-of-the-prior">Visualizing a one-step application of Bayes’ Theorem: the prominence of the prior</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-a-n-step-application-of-bayes-theorem-the-washing-out-of-the-priors">Visualizing a n-step application of Bayes’ Theorem: the washing out of the priors</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sample-space-and-probability-measure-in-bayes-theorem">The sample space and probability measure in Bayes’ Theorem</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="chapter-9">
<h1>Chapter 9<a class="headerlink" href="#chapter-9" title="Link to this heading">#</a></h1>
</section>
<section id="bayes-theorem-and-the-task-of-statistical-inference">
<h1>Bayes’ Theorem and the task of statistical inference<a class="headerlink" href="#bayes-theorem-and-the-task-of-statistical-inference" title="Link to this heading">#</a></h1>
<section id="the-task-of-statistical-inference">
<h2>The task of statistical inference<a class="headerlink" href="#the-task-of-statistical-inference" title="Link to this heading">#</a></h2>
<p>For the moment, suppose that we are working with only one random variable <span class="math notranslate nohighlight">\(X:\Omega\rightarrow \mathbb{R}\)</span>.</p>
<p>We are trying to use the value of <span class="math notranslate nohighlight">\(X(\omega)\)</span>, where <span class="math notranslate nohighlight">\(\omega\)</span> is the actual world, to get knowledge of what the distribution of <span class="math notranslate nohighlight">\(X\)</span> is.</p>
<p>In previous lectures we have focused on the cdf of <span class="math notranslate nohighlight">\(X\)</span>, but both it and its pdf determine the same information. Hence today let’s consider trying to decide on what the pdf of <span class="math notranslate nohighlight">\(X\)</span> is, i.e. what the answer, in the finite case, is to the question “How probable is it that <span class="math notranslate nohighlight">\(X\)</span> takes value <span class="math notranslate nohighlight">\(x\)</span>?”</p>
<p>Previously we were writing the pdf as <span class="math notranslate nohighlight">\(f\)</span>. In the Bayesian tradition, it is <span class="math notranslate nohighlight">\(\pi\)</span> or <span class="math notranslate nohighlight">\(p\)</span>, and so we use <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>We decide ahead of time that <span class="math notranslate nohighlight">\(X\sim p_{\theta}\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> comes from the parameter space <span class="math notranslate nohighlight">\(\Theta\)</span> and where usually <span class="math notranslate nohighlight">\(\{p_{\theta}: \theta \in \Theta\}\)</span> is some familar family like Bernoullis, binomials, or normals.</p>
<p>We’re trying to figure out which of the <span class="math notranslate nohighlight">\(\theta\)</span> from <span class="math notranslate nohighlight">\(\Theta\)</span> is such that <span class="math notranslate nohighlight">\(X\)</span> has pdf <span class="math notranslate nohighlight">\(p_{\theta}\)</span>.</p>
</section>
<section id="vivifying-bayes-theorem">
<h2>Vivifying Bayes’ Theorem<a class="headerlink" href="#vivifying-bayes-theorem" title="Link to this heading">#</a></h2>
<p>You decide to solve the problem by outsourcing it.</p>
<p><strong>Likelihood</strong>: You recruit a set <span class="math notranslate nohighlight">\(\Theta\)</span> of experts (or peers, or sources), where each <span class="math notranslate nohighlight">\(\theta\)</span> in <span class="math notranslate nohighlight">\(\Theta\)</span> thinks that <span class="math notranslate nohighlight">\(X\sim p_{\theta}\)</span>. You think of <span class="math notranslate nohighlight">\(p_{\theta}(x)\)</span> as how probable it is that <span class="math notranslate nohighlight">\(X=x\)</span>, assuming that <span class="math notranslate nohighlight">\(\theta\)</span> was right, i.e. assuming that <span class="math notranslate nohighlight">\(X\sim p_{\theta}\)</span>. Hence, you write <span class="math notranslate nohighlight">\(p(x\mid \theta)=p_{\theta}(x)\)</span> to indicate that it is the <em>likelihood</em> of <span class="math notranslate nohighlight">\(X=x\)</span> conditional on <span class="math notranslate nohighlight">\(X\sim p_{\theta}\)</span>.</p>
<p><strong>Prior (probability of hypothesis)</strong>: You yourself have some prior degrees of beliefs about which one of these experts is right. In particular, you have a probability measure <span class="math notranslate nohighlight">\(P\)</span> on <span class="math notranslate nohighlight">\(\Theta\)</span>, called <em>the prior</em>. You write <span class="math notranslate nohighlight">\(p(\theta)=P(\{\theta\})\)</span> for the probability that the hypothesis that <span class="math notranslate nohighlight">\(\theta\)</span> was right.</p>
<p><strong>(Probability of) evidence</strong>: You are seeking out evidence in the form of an observed event <span class="math notranslate nohighlight">\(X=x\)</span>. Prior to observation you assign the event <span class="math notranslate nohighlight">\(X=x\)</span> probability <span class="math notranslate nohighlight">\(p(x)=\sum_{\theta\in \Theta} p(\theta)\cdot p(x\mid \theta)\)</span>, i.e. you take the expectation of the random variable <span class="math notranslate nohighlight">\(Y(\theta) = p(x\mid \theta)\)</span>. That’s natural: after all, before you have updated, your views about the probability of <span class="math notranslate nohighlight">\(X=x\)</span> are being outsourced to the experts and <span class="math notranslate nohighlight">\(p(x)\)</span> is just averaging over their views according to the prior.</p>
<p>After observing some evidence <span class="math notranslate nohighlight">\(X(\omega)=x\)</span> in the actual world <span class="math notranslate nohighlight">\(\omega\)</span>, you update on the event <span class="math notranslate nohighlight">\(X=x\)</span>. You’re not gonna fire any of your experts for e.g. predicting that <span class="math notranslate nohighlight">\(X=x\)</span> had low probability when it actually happened, since low probability events do happen. But you are use this information to update your degrees of belief about which one of the experts is right.</p>
<p><strong>Posterior (probability of hypothesis)</strong>: You update in accord with Bayes’ Theorem to form the posterior <span class="math notranslate nohighlight">\(p(\theta \mid x)\)</span>, which is the probability you assign to “<span class="math notranslate nohighlight">\(\theta\)</span> being right” after having observed the event <span class="math notranslate nohighlight">\(X=x\)</span>. Hence, you use the formula:</p>
<div class="math notranslate nohighlight">
\[p(\theta \mid x) = \frac{p(x\mid \theta)\cdot p(\theta)}{p(x)}\]</div>
<p>which as per usual you remember in the mnemonic:</p>
<div class="math notranslate nohighlight">
\[\mathrm{posterior} = \frac{\mathrm{likelihood}\times \mathrm{prior}}{\mathrm{evidence}}\]</div>
<p><em>Note 1</em>: I am using “evidence” to help us adjust to this new application of Bayes’ Theorem. A more common term is <em>marginal distribution</em> or <em>prior predicative distribution</em>.</p>
<p><em>Note 2</em>: You might rightly wonder what the sample space is which makes this genuinely an application of Bayes’ Theorem. See last section of this chapter.</p>
</section>
<section id="the-components-of-bayes-theorem">
<h2>The components of Bayes’ Theorem<a class="headerlink" href="#the-components-of-bayes-theorem" title="Link to this heading">#</a></h2>
<p>Bayes’ Theorem takes the following form in the context of statistical inference:</p>
<div class="math notranslate nohighlight">
\[p(\theta \mid x) = \frac{p(x\mid \theta)\cdot p(\theta)}{p(x)}\]</div>
<p>which as per usual you remember in the mnemonic:</p>
<div class="math notranslate nohighlight">
\[\mathrm{posterior} = \frac{\mathrm{likelihood}\times \mathrm{prior}}{\mathrm{evidence}}\]</div>
<p>Here is a compact summary of the components:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>concept</p></th>
<th class="head"><p>notation</p></th>
<th class="head"><p>definition</p></th>
<th class="head"><p>how to find value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>prior</p></td>
<td><p><span class="math notranslate nohighlight">\(p(\theta)\)</span></p></td>
<td><p>prior degree of belief that <span class="math notranslate nohighlight">\(X\sim p_{\theta}\)</span></p></td>
<td><p>initially, consult your degrees of belief, which might be recorded in frequency information which you accept; after the initial round, the prior is the previous round’s posterior</p></td>
</tr>
<tr class="row-odd"><td><p>likelihood</p></td>
<td><p><span class="math notranslate nohighlight">\(p(x\mid \theta)\)</span></p></td>
<td><p>probability that <span class="math notranslate nohighlight">\(X=x\)</span> conditional on <span class="math notranslate nohighlight">\(X\sim p_{\theta}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(p_{\theta}(x)\)</span>, i.e. input in <span class="math notranslate nohighlight">\(x\)</span> to pdf <span class="math notranslate nohighlight">\(p_{\theta}\)</span>; look at graph or consult computer or book; if in applied context look at “if <span class="math notranslate nohighlight">\(\theta\)</span> were … then <span class="math notranslate nohighlight">\(x\)</span> would be …” statements salient in the subject matter</p></td>
</tr>
<tr class="row-even"><td><p>evidence (aka marginal, aka prior predictive distribution)</p></td>
<td><p><span class="math notranslate nohighlight">\(p(x)\)</span></p></td>
<td><p>probability that <span class="math notranslate nohighlight">\(X=x\)</span></p></td>
<td><p>use formula <span class="math notranslate nohighlight">\(p(x)=\sum_{\theta\in \Theta} p(\theta)\cdot p(x\mid \theta)\)</span>; in non-trivial cases use computer</p></td>
</tr>
<tr class="row-odd"><td><p>posterior</p></td>
<td><p><span class="math notranslate nohighlight">\(p(\theta \mid x)\)</span></p></td>
<td><p>probability that <span class="math notranslate nohighlight">\(X\sim p_{\theta}\)</span> conditional on <span class="math notranslate nohighlight">\(X=x\)</span></p></td>
<td><p>Use Bayes’ Theorem and three previous rows</p></td>
</tr>
</tbody>
</table>
</section>
<section id="advice-on-keeping-the-notation-straight">
<h2>Advice on keeping the notation straight:<a class="headerlink" href="#advice-on-keeping-the-notation-straight" title="Link to this heading">#</a></h2>
<p>When first encountering this perspective, the many distinct uses of <span class="math notranslate nohighlight">\(p\)</span> can be confusing.</p>
<p>The following remarks usually help one keep oneself straight:</p>
<ul class="simple">
<li><p>We use <span class="math notranslate nohighlight">\(x\)</span> to range over real numbers which the random variable <span class="math notranslate nohighlight">\(X\)</span> is outputting</p></li>
<li><p>We use <span class="math notranslate nohighlight">\(\theta\)</span> to range over the parameter space <span class="math notranslate nohighlight">\(\Theta\)</span>.</p></li>
<li><p>If you see <span class="math notranslate nohighlight">\(p_{\theta}\)</span> with <span class="math notranslate nohighlight">\(\theta\)</span> in subscript, this is the pdf associated to <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p>The other options are <span class="math notranslate nohighlight">\(p(x\mid \theta), p(x), p(\theta\mid x)\)</span>.</p></li>
<li><p>The two written in conditional notation are conditional probabilities of <span class="math notranslate nohighlight">\(X=x\)</span> conditional on <span class="math notranslate nohighlight">\(X\sim p_{\theta}\)</span> and vice-versa.</p></li>
<li><p>The final one, <span class="math notranslate nohighlight">\(p(x)\)</span>, is given by the formula up above, and is what you think about the probability of <span class="math notranslate nohighlight">\(X=x\)</span> before updating, namely by taking the expectation of the random variable <span class="math notranslate nohighlight">\(Y(\theta) = p(x\mid \theta)\)</span>.</p></li>
</ul>
</section>
<section id="a-simple-example-uniform-vs-three-normals-plus-prior-which-is-uniform">
<h2>A simple example: uniform vs. three normals, plus prior which is uniform<a class="headerlink" href="#a-simple-example-uniform-vs-three-normals-plus-prior-which-is-uniform" title="Link to this heading">#</a></h2>
<p>You recruit four experts: <span class="math notranslate nohighlight">\(\theta_0, \theta_1, \theta_2, \theta_3\)</span>.</p>
<p>They are so-named because expert <span class="math notranslate nohighlight">\(\theta_i\)</span> thinks tha <span class="math notranslate nohighlight">\(X\sim \theta_i\)</span>.</p>
<p><strong>Likelihood</strong>: one of your experts <span class="math notranslate nohighlight">\(\theta_0\)</span> uses the uniform measure and thinks that every outcome is equally likely. The other three <span class="math notranslate nohighlight">\(\theta_1, \theta_2, \theta_3\)</span> think that the random variable’s probability distribution accords with a normal distribution, but they disagree about what the mean and variance are.</p>
<p><strong>Prior</strong>: you just hired these four experts and don’t have much to go on, so you just say that it is equally likely that each is right. That is, you say each has <span class="math notranslate nohighlight">\(1/4\)</span> chance of being right. Hence, you set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">my_prior_1</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.25</span><span class="p">,</span> <span class="mf">.25</span><span class="p">,</span> <span class="mf">.25</span><span class="p">,</span> <span class="mf">.25</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Evidence</strong>: for all <span class="math notranslate nohighlight">\(x\)</span>, you diligently calculate the probability <span class="math notranslate nohighlight">\(p(x)=.25*p(x\mid \theta_0)+.25*p(x\mid \theta_1)+.25*p(x\mid \theta_2)+.25*p(x\mid \theta_4)\)</span>.</p>
<p><strong>Posterior</strong>: you use Bayes’ Theorem to calculate <span class="math notranslate nohighlight">\(p(\theta_0 \mid x), p(\theta_1 \mid x), p(\theta_2 \mid x), p(\theta_3 \mid x)\)</span>, which is is how probable you think it is that the experts are right, after you have seen how they preformed on this first piece of data <span class="math notranslate nohighlight">\(X=x\)</span>.</p>
</section>
</section>
<section id="visualizing-a-one-step-application-of-bayes-theorem-the-prominence-of-the-prior">
<h1>Visualizing a one-step application of Bayes’ Theorem: the prominence of the prior<a class="headerlink" href="#visualizing-a-one-step-application-of-bayes-theorem-the-prominence-of-the-prior" title="Link to this heading">#</a></h1>
<p>Below is the graph, and below it a little commentary, geared towards understanding the effects of the different components of Bayes’ Theorem.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>   <span class="c1"># load pyplot package</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># size_sample_space, size_parameter_space are positive integers</span>
<span class="c1"># prior is list of length size_parameter_space of non-negative numbers that sum to 1</span>
<span class="c1"># likelihood is a list of length size_parameter_space </span>
<span class="c1"># where each element is a list of length size_sample_space numbers that sum to 1</span>

<span class="k">def</span> <span class="nf">bayes</span><span class="p">(</span><span class="n">size_sample_space</span><span class="p">,</span><span class="n">size_parameter_space</span><span class="p">,</span><span class="n">prior</span><span class="p">,</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">print_table</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">observed_value</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">round_digits</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.98</span> <span class="o">&lt;=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">1.02</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The sum of the prior probabilities must be equal to 1&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span> <span class="o">!=</span> <span class="n">size_parameter_space</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The length of the prior probabilities must be equal to the size of the parameter space&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">likelihood</span><span class="p">)</span> <span class="o">!=</span> <span class="n">size_parameter_space</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The length of the likelihood must be equal to the size of the parameter space&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">likelihood</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">!=</span> <span class="n">size_sample_space</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size_parameter_space</span><span class="p">)]):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The length of each entry in the likelihood must be equal to the size of the sample space&quot;</span><span class="p">)</span>
  

    <span class="n">sample_space</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">size_sample_space</span><span class="p">))</span>

    <span class="n">parameter_space</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">size_parameter_space</span><span class="p">))</span>



    <span class="n">parameter_space_names</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_space</span><span class="p">:</span>
        <span class="n">parameter_space_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;likelihood p(x_j  &amp;#124; θ_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="n">evidence</span> <span class="o">=</span>  <span class="p">[]</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">sample_space</span><span class="p">:</span>
        <span class="n">evidence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">([</span><span class="n">prior</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">likelihood</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_space</span><span class="p">]))</span>

    <span class="n">sample_space_names</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">sample_space</span><span class="p">:</span>
        <span class="n">sample_space_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x_</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">, evidence = </span><span class="si">{</span><span class="n">evidence</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">round_digits</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">likelihood</span><span class="p">)</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">T</span>

    <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">parameter_space_names</span>

    <span class="n">df</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">sample_space_names</span>

    <span class="n">posterior</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="n">size_sample_space</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size_parameter_space</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_space</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">sample_space</span><span class="p">:</span>
            <span class="n">posterior</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">((</span><span class="n">likelihood</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">prior</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="n">evidence</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_space</span><span class="p">:</span>
        <span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;posterior p(θ_ </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">  &amp;#124; x_j)&quot;</span><span class="p">,</span> <span class="n">posterior</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>


    <span class="n">parameter_space_names</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_space</span><span class="p">:</span>
        <span class="n">parameter_space_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;likelihood p(x_j  &amp;#124; θ_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

    <span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">round_digits</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">print_table</span><span class="p">:</span>
        <span class="n">markdown_table</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">to_markdown</span><span class="p">()</span>

        <span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">markdown_table</span><span class="p">))</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>  <span class="c1"># 1 row, 2 columns</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_space</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_space</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;θ_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_space</span><span class="p">,</span> <span class="n">evidence</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;evidence&#39;</span><span class="p">)</span>        
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">likelihood</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">likelihood</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="o">+</span><span class="mf">.001</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;likelihood p(x | θ)&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_space</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_space</span><span class="p">,</span> <span class="n">posterior</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;θ_</span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">observed_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">observed_value</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;observed value&#39;</span><span class="p">)</span>       
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;posterior p(θ  | x)&#39;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>

    <span class="n">prior_suptitle</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;θ_</span><span class="si">%i</span><span class="s1"> = </span><span class="si">%1.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">prior</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_space</span><span class="p">]</span>
    <span class="n">prior_suptitle_string</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">prior_suptitle</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">.8</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;likelihood vs. posterior with prior </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">prior_suptitle_string</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">posterior</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bayes_three_normal_vs_uniform</span><span class="p">(</span><span class="n">size_sample_space</span><span class="p">,</span><span class="n">prior</span><span class="p">,</span> <span class="n">mean_1</span><span class="p">,</span><span class="n">var_1</span><span class="p">,</span><span class="n">mean_2</span><span class="p">,</span><span class="n">var_2</span><span class="p">,</span><span class="n">mean_3</span><span class="p">,</span><span class="n">var_3</span><span class="p">,</span> <span class="n">observed_value</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">round_digits</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>

    <span class="n">my_likelihood</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>

    <span class="n">my_likelihood</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="n">size_sample_space</span><span class="p">]</span><span class="o">*</span><span class="n">size_sample_space</span>

    <span class="n">std_dev_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_1</span><span class="p">)</span>
    <span class="n">std_dev_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_2</span><span class="p">)</span>
    <span class="n">std_dev_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_3</span><span class="p">)</span>

    <span class="n">my_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">mean_1</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_1</span><span class="p">,</span> <span class="n">mean_2</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_2</span><span class="p">,</span> <span class="n">mean_3</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_3</span><span class="p">)</span>
    <span class="n">my_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">mean_1</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_1</span><span class="p">,</span> <span class="n">mean_2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_2</span><span class="p">,</span> <span class="n">mean_3</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_3</span><span class="p">)</span>

    <span class="n">my_sample_space_alt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">my_min</span><span class="p">,</span> <span class="n">my_max</span><span class="p">,</span> <span class="n">size_sample_space</span><span class="p">)</span>

    <span class="n">my_likelihood</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">my_sample_space_alt</span><span class="p">,</span> <span class="n">mean_1</span><span class="p">,</span> <span class="n">std_dev_1</span><span class="p">)</span>
    <span class="n">my_likelihood</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">my_sample_space_alt</span><span class="p">,</span> <span class="n">mean_2</span><span class="p">,</span> <span class="n">std_dev_2</span><span class="p">)</span>
    <span class="n">my_likelihood</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">my_sample_space_alt</span><span class="p">,</span> <span class="n">mean_3</span><span class="p">,</span> <span class="n">std_dev_3</span><span class="p">)</span>

    <span class="n">bayes</span><span class="p">(</span><span class="n">size_sample_space</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">prior</span><span class="p">,</span><span class="n">my_likelihood</span><span class="p">,</span> <span class="n">print_table</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">observed_value</span> <span class="o">=</span> <span class="n">observed_value</span><span class="p">)</span>
    
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bayes_three_normal_vs_uniform</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="n">my_prior_1</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/d8c1c756c73a366d7f6644538770dcdf741da8a88764280969d152273b573add.png" src="_images/d8c1c756c73a366d7f6644538770dcdf741da8a88764280969d152273b573add.png" />
</div>
</div>
<p>In the above graph, on the left we have:</p>
<ul class="simple">
<li><p>pdf of <span class="math notranslate nohighlight">\(\theta_0\)</span> (blue) the uniform distribution, a straight line with low values for each entry</p></li>
<li><p>pdf of <span class="math notranslate nohighlight">\(\theta_1\sim N(30, 1)\)</span> (orange), <span class="math notranslate nohighlight">\(\theta_2\sim N(40, 2)\)</span> (green), <span class="math notranslate nohighlight">\(\theta_3\sim N(30,10)\)</span> (red) are the three normals</p></li>
<li><p>the evidence <span class="math notranslate nohighlight">\(p(x)\)</span>, in the dashed line, which is just what your estimate of how probable <span class="math notranslate nohighlight">\(x\)</span> is before you update</p></li>
</ul>
<p>In the above graph, on the right we have:</p>
<ul class="simple">
<li><p>the posterior <span class="math notranslate nohighlight">\(p(\theta\mid x)\)</span>: once you fix an observed value <span class="math notranslate nohighlight">\(X=x\)</span> on the <span class="math notranslate nohighlight">\(x\)</span>-axis, how probable it is that expert <span class="math notranslate nohighlight">\(\theta\)</span> was right that <span class="math notranslate nohighlight">\(X\sim p_{\theta}\)</span>.</p></li>
</ul>
<p>Note that the graph on the right is not a pdf of the sample space. Rather, if you fix an <span class="math notranslate nohighlight">\(x\)</span>-value, the value of the four lines at that <span class="math notranslate nohighlight">\(x\)</span>-value add up to one, and describe a pdf of the parameter space.</p>
<p>Note the following about the graphs:</p>
<ul class="simple">
<li><p>If one looks at <span class="math notranslate nohighlight">\(x=18\)</span>, one sees that the orange expert <span class="math notranslate nohighlight">\(\theta_1\)</span> thought that the event <span class="math notranslate nohighlight">\(X=x\)</span> was much higher than the red expert <span class="math notranslate nohighlight">\(\theta_3\)</span>. Since we think that each expert has a <span class="math notranslate nohighlight">\(1/4\)</span> chance of being right and since the probability of the evidence doesn’t differ between the experts, we see from Bayes’ Theorem that the posterior of the orange expert <span class="math notranslate nohighlight">\(\theta_1\)</span> is much higher than the posterior of the red expert <span class="math notranslate nohighlight">\(\theta_3\)</span>.</p></li>
<li><p>If one looks at the shape of the orange expert <span class="math notranslate nohighlight">\(\theta_1\)</span> between the graphs on the left and the right, they look kind of the same. But if one look at the shape of the red expert <span class="math notranslate nohighlight">\(\theta_3\)</span>, the graphs on the left and the right look really different. The explanation is that as we move closer to <span class="math notranslate nohighlight">\(x=18\)</span>, the probability <span class="math notranslate nohighlight">\(p(x)\)</span> in the dashed line is getting large because the orange expert <span class="math notranslate nohighlight">\(\theta_1\)</span> is thinking that it is so probable. Since we are dividing by this quantity in calculating the red <span class="math notranslate nohighlight">\(p(\theta_3\mid x)\)</span>, we are getting a smaller number. This doesn’t happen for the orange <span class="math notranslate nohighlight">\(p(\theta_1\mid x)\)</span> because the numerator also includes the large values of <span class="math notranslate nohighlight">\(p(x\mid \theta_1)\)</span>.</p></li>
<li><p>Expert <span class="math notranslate nohighlight">\(\theta_0\)</span> is adopting the uniform measure and thinks that all outcomes are equally likely. But note that the posterior <span class="math notranslate nohighlight">\(p(\theta_0\mid x)\)</span> varies a whole lot depending on <span class="math notranslate nohighlight">\(x\)</span>. This is because this value is being affected, in Bayes’ Theorem, by dividing by <span class="math notranslate nohighlight">\(p(x)\)</span>, which varies greatly with <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
<p>Suppose now that you change the prior, say to:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">my_prior_2</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">.15</span><span class="p">,</span> <span class="mf">.30</span><span class="p">,</span> <span class="mf">.05</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>That which one gets is the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bayes_three_normal_vs_uniform</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="n">my_prior_2</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/8b9a0891bb275a2b99d6a897fd5eeb94bbe69ecaf9c8464e873af8244ed5b670.png" src="_images/8b9a0891bb275a2b99d6a897fd5eeb94bbe69ecaf9c8464e873af8244ed5b670.png" />
</div>
</div>
<p>This dependence on the prior is viewed by many non-Bayesians as a deep problem. Here is Mayo:</p>
<blockquote>
<div><p>[The] question for the subjective Bayesian is whether scientists have prior degrees of belief in the hypotheses they investigate and whether, even if they do, it is desirable to have them figure centrally in learning from data in science. In science, it seems, we want to know what the data are saying, quite apart from the opinions we start out with (<span id="id1">[<a class="reference internal" href="references.html#id34" title="Deborah G Mayo. Error and the Growth of Experimental Knowledge. University of Chicago Press, August 1996.">Mayo, 1996</a>]</span> p. 76)</p>
</div></blockquote>
<p>She is not alone in this, and even many Bayesians feel the need to response to this objection. Our graphs up above make vivid this problem by way of one specific example. To feel the force of the concern, imagine that a lot is at stake in the outcome of the decision as to what the posterior should be, and ask yourself whether it is okay for scientific (or other purposes) for there to be so much dependence on the prior.</p>
<section id="visualizing-a-n-step-application-of-bayes-theorem-the-washing-out-of-the-priors">
<h2>Visualizing a n-step application of Bayes’ Theorem: the washing out of the priors<a class="headerlink" href="#visualizing-a-n-step-application-of-bayes-theorem-the-washing-out-of-the-priors" title="Link to this heading">#</a></h2>
<p>Howson and Urbach’s preferred response to this problem is the following:</p>
<blockquote>
<div><p>[…] scientific opinions often do converge, often indeed, very quickly, after relatively little data (<span id="id2">[<a class="reference internal" href="references.html#id35" title="Colin Howson and Peter Urbach. Scientific Reasoning: The Bayesian Approach. Open Court Publishing, 2006.">Howson and Urbach, 2006</a>]</span> p. 238)</p>
</div></blockquote>
<blockquote>
<div><p>Moreover – and this is the crucial point for the explaining the objectivity of inference– the objective information contained in the sample becomes the dominant factor relatively quickly (<span id="id3">[<a class="reference internal" href="references.html#id35" title="Colin Howson and Peter Urbach. Scientific Reasoning: The Bayesian Approach. Open Court Publishing, 2006.">Howson and Urbach, 2006</a>]</span> p. 240)</p>
</div></blockquote>
<p>In the first quotation the idea is that the dependence on the prior is not a problem because agents with different priors will quickly come to agree with one another, and so the salve for the subjectivity of the prior is the intersubjectivity of posterior after a few rounds. In the second quotation, the idea is that the data observed is explanatory of this intersubjective agreement and hence is the source of the objectivity in the Bayesian approach.</p>
<p>Howson and Urbach were writing in the period before easily accessible statistical software. They buttress their points by considering specific distributions (normals, binomials) and by assuming that the prior is also from that class and that the evidence one is sampling is independent. But there is a more general point available here that can be described by way of illustration with graphs.</p>
<p>Suppose that we observe many values and repeatedly apply Bayes’ Theorem.</p>
<p>For instance, suppose that we observe:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">my_data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Then we can visualize what we think about our two experts, first with our original prior:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;my_prior_1: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">my_prior_1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>my_prior_1: [0.25, 0.25, 0.25, 0.25]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bayes_pie</span><span class="p">(</span><span class="n">size_sample_space</span><span class="p">,</span><span class="n">size_parameter_space</span><span class="p">,</span><span class="n">prior</span><span class="p">,</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">observed_values</span><span class="p">):</span>

    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">observed_values</span><span class="p">)</span>

    <span class="n">parameter_space</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">size_parameter_space</span><span class="p">))</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span> 

    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;θ_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size_parameter_space</span><span class="p">)]</span>  

    <span class="n">prior_0</span> <span class="o">=</span> <span class="n">prior</span>

    <span class="n">prior_suptitle</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;θ_</span><span class="si">%i</span><span class="s1"> = </span><span class="si">%1.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">prior_0</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">parameter_space</span><span class="p">]</span>
    <span class="n">prior_suptitle_string</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">prior_suptitle</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;posterior_n for n≤</span><span class="si">%i</span><span class="s1">, with prior </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">prior_suptitle_string</span><span class="p">))</span>    

    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">pie</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;prior&#39;</span><span class="p">)</span>    

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>

        <span class="n">evidence</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">prior</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">likelihood</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">observed_values</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size_parameter_space</span><span class="p">)])</span>

        <span class="n">posterior</span> <span class="o">=</span> <span class="p">[</span><span class="n">prior</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">likelihood</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">observed_values</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span><span class="o">/</span><span class="n">evidence</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size_parameter_space</span><span class="p">)]</span>

        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pie</span><span class="p">(</span><span class="n">posterior</span><span class="p">)</span>

        <span class="n">axs</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;posterior_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

        <span class="n">prior</span> <span class="o">=</span> <span class="n">posterior</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center left&#39;</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">1.25</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">posterior</span>

    
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bayes_pie_three_normal_vs_uniform</span><span class="p">(</span><span class="n">size_sample_space</span><span class="p">,</span><span class="n">prior</span><span class="p">,</span> <span class="n">mean_1</span><span class="p">,</span><span class="n">var_1</span><span class="p">,</span><span class="n">mean_2</span><span class="p">,</span><span class="n">var_2</span><span class="p">,</span><span class="n">mean_3</span><span class="p">,</span><span class="n">var_3</span><span class="p">,</span> <span class="n">observed_values</span><span class="p">):</span>

    <span class="n">my_likelihood</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>

    <span class="n">my_likelihood</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="n">size_sample_space</span><span class="p">]</span><span class="o">*</span><span class="n">size_sample_space</span>

    <span class="n">std_dev_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_1</span><span class="p">)</span>
    <span class="n">std_dev_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_2</span><span class="p">)</span>
    <span class="n">std_dev_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_3</span><span class="p">)</span>

    <span class="n">my_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">mean_1</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_1</span><span class="p">,</span> <span class="n">mean_2</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_2</span><span class="p">,</span> <span class="n">mean_3</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_3</span><span class="p">)</span>
    <span class="n">my_max</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">mean_1</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_1</span><span class="p">,</span> <span class="n">mean_2</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_2</span><span class="p">,</span> <span class="n">mean_3</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">std_dev_3</span><span class="p">)</span>

    <span class="n">my_sample_space_alt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">my_min</span><span class="p">,</span> <span class="n">my_max</span><span class="p">,</span> <span class="n">size_sample_space</span><span class="p">)</span>

    <span class="n">my_likelihood</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">my_sample_space_alt</span><span class="p">,</span> <span class="n">mean_1</span><span class="p">,</span> <span class="n">std_dev_1</span><span class="p">)</span>
    <span class="n">my_likelihood</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">my_sample_space_alt</span><span class="p">,</span> <span class="n">mean_2</span><span class="p">,</span> <span class="n">std_dev_2</span><span class="p">)</span>
    <span class="n">my_likelihood</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">my_sample_space_alt</span><span class="p">,</span> <span class="n">mean_3</span><span class="p">,</span> <span class="n">std_dev_3</span><span class="p">)</span>

    <span class="n">bayes_pie</span><span class="p">(</span><span class="n">size_sample_space</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">my_likelihood</span><span class="p">,</span> <span class="n">observed_values</span><span class="p">)</span>
    
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bayes_pie_three_normal_vs_uniform</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="n">my_prior_1</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="n">my_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/cfebcf8c06fc3b8b38a44123bc2cfea70e46fb6f33bcda44219e13d9d5f86b09.png" src="_images/cfebcf8c06fc3b8b38a44123bc2cfea70e46fb6f33bcda44219e13d9d5f86b09.png" />
</div>
</div>
<p>And then with the alternative prior:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;my_prior_2: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">my_prior_2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>my_prior_2: [0.5, 0.15, 0.3, 0.05]
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bayes_pie_three_normal_vs_uniform</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="n">my_prior_2</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="n">my_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/d67b9a5b75ef9bb13a6a7fd1bbf36ae53f849799440076e14909b36f71a369fa.png" src="_images/d67b9a5b75ef9bb13a6a7fd1bbf36ae53f849799440076e14909b36f71a369fa.png" />
</div>
</div>
</section>
</section>
<section id="the-sample-space-and-probability-measure-in-bayes-theorem">
<h1>The sample space and probability measure in Bayes’ Theorem<a class="headerlink" href="#the-sample-space-and-probability-measure-in-bayes-theorem" title="Link to this heading">#</a></h1>
<p>In this brief section, we argue that our application of Bayes’ Theorem is indeed an application of the Theorem in the more famliar sense, at least in the finite case. That is, we need to explicitly describe what the spaces and probability measures are which are issue. One can give a similar argument in the infinite case, but it comes with additional hypotheses to effect that all the <span class="math notranslate nohighlight">\(p_{\theta}\)</span> agree about what is possible and impossible (i.e. what has probability zero and what has probability one).</p>
<p>Suppose that <span class="math notranslate nohighlight">\(\Omega\)</span> and <span class="math notranslate nohighlight">\(\Theta\)</span> are finite.</p>
<p>Then <span class="math notranslate nohighlight">\(X:\Omega \rightarrow \mathbb{R}\)</span> has finite range <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(R=\{x_1, \ldots, x_n\}\)</span>.</p>
<p>Consider the product space <span class="math notranslate nohighlight">\(R\times \Theta\)</span>.</p>
<p>Define a probability measure <span class="math notranslate nohighlight">\(Pr\)</span> on this space by <span class="math notranslate nohighlight">\(Pr(\{x_i\}\times \{\theta\})=p(\theta)\cdot p(x_i \mid \theta)\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(x, \theta\)</span> be fixed, and let <span class="math notranslate nohighlight">\(H_{\theta}=R\times \{\theta\}\)</span> and let <span class="math notranslate nohighlight">\(E_{x}=\{x\}\times \Theta\)</span>. Then one has</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Pr(H_{\theta})=p(\theta)\)</span> because <span class="math notranslate nohighlight">\(Pr(H_{\theta})=Pr(R\times \{\theta\}) = \sum_{i=1}^n p(\theta)\cdot p(x_i \mid \theta) = p(\theta) \cdot ( \sum_{i=1}^n p_{\theta}(x_i) ) = p(\theta)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Pr(E_{x}\mid H_{\theta})=p(x\mid \theta)\)</span> because <span class="math notranslate nohighlight">\(Pr(E_{x}\mid H_{\theta}) = \frac{Pr(\{x\}\times \{\theta\})}{Pr(H_{\theta})} = \frac{p(\theta)\cdot p(x \mid \theta)}{p(\theta)} = p(x\mid \theta)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Pr(E_{x})=p(x)\)</span> because <span class="math notranslate nohighlight">\(Pr(E_{x})=Pr(\{x\}\times \Theta)=\sum_{\theta\in \Theta} Pr(\{x\}\times \{\theta\})=\sum_{\theta\in \Theta} p(\theta)\cdot p(x \mid \theta)=p(x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Pr(H_{\theta} \mid E_{x})=\frac{p(\theta)\cdot p(x \mid \theta)}{p(x)}\)</span> either by Bayes’ Theorem from the previous lines, or by direct calculation from <span class="math notranslate nohighlight">\(Pr(H_{\theta} \mid E_{x})=\frac{Pr(\{x\}\times \{\theta\})}{Pr(E_{x})} = \frac{p(\theta)\cdot p(x \mid \theta)}{p(x)}\)</span>.</p></li>
</ul>
<p>Hence if we then define <span class="math notranslate nohighlight">\(p(\theta\mid x)=Pr(H_{\theta} \mid E_{x})\)</span>, then we get the desired result that</p>
<div class="math notranslate nohighlight">
\[p(\theta\mid x) =\frac{p(\theta)\cdot p(x \mid \theta)}{p(x)}\]</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Chap08.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 8</p>
      </div>
    </a>
    <a class="right-next"
       href="Homework02.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Homework 2</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 9</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-and-the-task-of-statistical-inference">Bayes’ Theorem and the task of statistical inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-task-of-statistical-inference">The task of statistical inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vivifying-bayes-theorem">Vivifying Bayes’ Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-components-of-bayes-theorem">The components of Bayes’ Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advice-on-keeping-the-notation-straight">Advice on keeping the notation straight:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-example-uniform-vs-three-normals-plus-prior-which-is-uniform">A simple example: uniform vs. three normals, plus prior which is uniform</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-a-one-step-application-of-bayes-theorem-the-prominence-of-the-prior">Visualizing a one-step application of Bayes’ Theorem: the prominence of the prior</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-a-n-step-application-of-bayes-theorem-the-washing-out-of-the-priors">Visualizing a n-step application of Bayes’ Theorem: the washing out of the priors</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sample-space-and-probability-measure-in-bayes-theorem">The sample space and probability measure in Bayes’ Theorem</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sean Walsh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>