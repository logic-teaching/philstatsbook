{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10\n",
    "\n",
    "Today we look at Mayo's recent discussion {cite}`Mayo2018-qa` of a proposal by Kass {cite}`Kass2011-yj` about how to reconcile the classical and Bayesian approaches. \n",
    "\n",
    "Kass's suggestion is as follows:\n",
    "\n",
    "> The commonality between frequentist and Bayesian inferences is the use of theoretical assumptions, together with a *subjunctive* statement. In both approaches a statistical model is introduced—in the Bayesian case the prior distributions become part of what I am here calling the model—and we may say that the inference is based on what *would* happen if the data *were* to be random variables distributed according to the statistical model. This modeling assumption would be reasonable if the model *were* to describe accurately the variation in the data ( {cite}`Kass2011-yj` pp. 4-5)\n",
    "\n",
    "As we will see, Mayo rejects this both for her own way of thinking about the classical school, and she further thinks that the more austere Bayesian would reject it as well.\n",
    "\n",
    "In addition to being an important recent discussion, going through this debate will help us see better the differences between the classical and Bayesian school."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classical inference\n",
    "\n",
    "### A classical inference with low probability of type I error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shapely'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwidgets\u001b[39;00m    \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interact, interactive, fixed, interact_manual, FloatSlider, IntSlider\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LineString\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shapely'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import binom, norm\n",
    "from scipy.integrate import quad\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt   \n",
    "import metakernel\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import ipywidgets as widgets    \n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider, IntSlider\n",
    "from shapely.geometry import LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def normalvisual(mu, var, n):\n",
    "\n",
    "    varavg = var / n  # variance of the average\n",
    "\n",
    "    # standard deviation of the average\n",
    "\n",
    "    sigma = np.sqrt(var)\n",
    "    sigmaavg = np.sqrt(varavg)\n",
    "\n",
    "    # Create a range\n",
    "    x = np.linspace(0, 15, 10000)\n",
    "    l = arr = np.full(len(x), .05)\n",
    "\n",
    "    # Create the normal distribution for the range\n",
    "    y = norm.pdf(x, mu, sigma)\n",
    "    yavg = norm.pdf(x, mu, sigmaavg)\n",
    "    tavg = 1-norm.cdf(mu+x, mu, sigmaavg)+norm.cdf(mu-x, mu, sigmaavg)\n",
    "\n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot the normal distribution on the left subplot\n",
    "    axs[0].plot(x, yavg)\n",
    "    axs[1].plot(x, tavg)\n",
    "    axs[0].set_xlim(9, 15)\n",
    "    axs[1].set_xlim(0, 1)\n",
    "    axs[1].set_ylim(0, 1)    \n",
    "    axs[1].plot(x, l, label='.05', color='r')\n",
    "\n",
    "\n",
    "    # Plot the histogram on the right subplot\n",
    "    # axs[1].hist(data, bins=30, density=True, alpha=0.6, color='g')\n",
    "\n",
    "    # Set titles\n",
    "    axs[0].set_title('pdf of X̅_{%i}~N(%1.1f, %1.3f)' % (n, mu, varavg))\n",
    "    axs[1].set_title('probability of |X̅_{%i}-%1.1f|≥c, \\n as function of c, \\n (aka probability of Type I error \\n as function of c)' % (n, mu))\n",
    "    axs[1].set_xlabel('c')  # Add x-axis label\n",
    "\n",
    "    first_line = LineString(np.column_stack((x, tavg)))\n",
    "    second_line = LineString(np.column_stack((x, l)))\n",
    "    intersection = first_line.intersection(second_line)\n",
    "\n",
    "    # ... existing code ...\n",
    "\n",
    "    intersection = first_line.intersection(second_line)\n",
    "\n",
    "    axs[1].axvline(x=intersection.x, color='g', label='%1.2f' % intersection.x)\n",
    "    axs[1].legend()\n",
    "    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalling a hypothesis test from the classical school\n",
    "\n",
    "We are repeatedly and independently measure $X_1, \\ldots, X_n$ which we assume to be normally distributed with common unknown mean and known variance. For the sake of concreteness, suppose that the variance is 1.\n",
    "\n",
    "We want to know the value of $\\mu$ such that $X_1, \\ldots, X_n\\sim N(\\mu,1)$.\n",
    "\n",
    "That is, we imagine we already know that the variance is 1, but we want to know where the center of the bell shape is.\n",
    "\n",
    "We formulate the hypothesis that the center is at $\\mu_0$:\n",
    "\n",
    "- The null hypothesis $H_0$: $\\hspace{5mm}$ $\\mu=\\mu_0$\n",
    "\n",
    "- The alternative hypothesis $H_1$: $\\hspace{5mm}$ $\\mu \\neq \\mu_0$. \n",
    "\n",
    "We choose a comparatively large value $c>0$, and we adopt the following test:\n",
    "\n",
    "- Test: reject $H_0$ if $T_n \\geq c$.\n",
    "\n",
    "where $T_n =\\left|\\overline{X}_n - \\mu_0\\right|$ and $\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$ is the average\n",
    "\n",
    "If $c$ is large and the null hypothesis is true, then the event $T_n\\geq c$ has small probability and thus the Type I error is low. \n",
    "\n",
    "In the special case of normals with fixed variance and fixed $n$, the probability of the Type I error is constant across all choices of the null hypothesis, as the following examples illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8eb80d966b342299e8c774163229afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=10.2, description='mu', max=20.0), FloatSlider(value=1.0, description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.normalvisual(mu, var, n)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(normalvisual, \n",
    "         mu=FloatSlider(min=0, max=20, step=0.1, value=10.2), \n",
    "         var=FloatSlider(min=.1, max=10, step=0.1, value=1), \n",
    "         n=FloatSlider(min=1, max=100, step=0.1, value=49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc19bd1c20824310b56e4c2510fade9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=12.0, description='mu', max=20.0), FloatSlider(value=1.0, description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.normalvisual(mu, var, n)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(normalvisual, \n",
    "         mu=FloatSlider(min=0, max=20, step=0.1, value=12), \n",
    "         var=FloatSlider(min=.1, max=10, step=0.1, value=1), \n",
    "         n=FloatSlider(min=1, max=100, step=0.1, value=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later reference, we record some values, where the value of $c$ is the smallest value such that the probability of a type I error in the test \"reject $H_0$ if $T_n\\geq c$\" is $\\leq .05$:\n",
    "\n",
    "|  | variance | $n$ | $c$ |\n",
    "|----------|----------|----------|----------|\n",
    "| Example 1   | 1   | 49   | .28   |\n",
    "| Example 2  |  1  | 100   | .20   |\n",
    "| Example 3  |  4 | 100  | .40  |\n",
    "| Example 4  | 4  | 49  | .56  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recasting this classical inference as a confidence interval\n",
    "\n",
    "Suppose that we choose $c$ optimally, right where the type I probability of the event $T_n\\geq c$ falls below $.05$. In the above examples, it is $c=.28$.\n",
    "\n",
    "Hence one has the probability of the event $T_n<c$ is above $.95$.\n",
    "\n",
    "But one has $T_n<c$ \n",
    "\n",
    "iff $-c<\\mu_0-\\overline{X}_n<c$ \n",
    "     \n",
    "iff $\\overline{X}_n-c<\\mu_0<\\overline{X}_n+c$.\n",
    "\n",
    "Hence the probability that \" '$\\pm$ c of the average' contains the null hypothesis\" is above $.95$. \n",
    "\n",
    "And this holds regardless of what the null hypothesis is.\n",
    "\n",
    "We recopy our table, noting now that $c$ is the smallest value such that the probability that $\\overline{X}_n-c<\\mu_0<\\overline{X}_n+c$ is above $.95$.\n",
    "\n",
    "|  | variance | $n$ | $c$ |\n",
    "|----------|----------|----------|----------|\n",
    "| Example 1   | 1   | 49   | .28   |\n",
    "| Example 2  |  1  | 100   | .20   |\n",
    "| Example 3  |  4 | 100  | .40  |\n",
    "| Example 4  | 4  | 49  | .56  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kass and Mayo on this classical inference\n",
    "\n",
    "Kass {cite}`Kass2011-yj` describes how the classical school would view this inference:\n",
    "\n",
    "> *Frequentist interpretation of the confidence interval.* Under the assumptions above, if we were to draw infinitely many random samples from a $N(\\mu_0,1)$ distribution, 95% of the corresponding confidence intervals $(\\overline{X}_{49}-.28, \\overline{X}_{49}+.28)$ would cover $\\mu_0$ ({cite}`Kass2011-yj` p. 4, notation changed to match preceeding discussion)\n",
    "\n",
    "Kass contrast this to his preferred 'pragmatist' approach:\n",
    "\n",
    "> *Frequentist interpretation of the confidence interval.* If we were to draw a random sample according to the assumptions above, the resulting confidence interval $(\\overline{X}_{49}-.28, \\overline{X}_{49}+.28)$ would have probability 0.95 of covering μ. Because the random sample lives in the theoretical world, this is a theoretical statement. Nonetheless, substituting $\\overline{X}_{49}=10.2$, we obtain the interval $(10.2-.28, 10.2+.28)$, and are able to draw useful conclusions as long as our theoretical world is aligned well with the real world that produced the data ({cite}`Kass2011-yj` p. 4, notation changed to match preceeding discussion)\n",
    "\n",
    "Mayo critizes Kass' pragmatist approach and offers her own way of thinking about the confidence interval:\n",
    "\n",
    "> Everything is left in the subjunctive conditional: if the 'theoretical world is aligned well with the real world that produced the data' then such and such follows. [P] Would the frequentist [...] be content with this? The frequentist error statistican would not, because there's no detachment of an inference ({cite}`Mayo2018-qa` pp. 426-427)\n",
    "\n",
    "I take it that she means that Kass' pragmatist hasn't given us an independent reason to think that the  theoretical world is aligned well with the real world, and that it is part of the task of statistical inference to help us towards that knowledge. \n",
    "\n",
    "Here is Mayo's own view about how to think about the confidence intervals:\n",
    "\n",
    "> Kass' 0.95 interval estimate is $(9.92, 10.48)$ [coming from (10.2-.28, 10.2+.28)$]. There's a good indication that $\\mu_0>9.92$ because if $\\mu$ were 9.92 (or lower), we very frequently would have gotten a smaller value of $\\overline{X}_n$ than we did. [...] What would occur in hypothetical repetitions, under various claims about parameters, is not for future assurance in using the rule, but ot understand what caused the events on which this inference is based. If a method is incapable of reflecting changes in the data-generating source, its inferences are criticized on grounds of severity. ({cite}`Mayo2018-qa` p. 429)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last word on confidence intervals\n",
    "\n",
    "Recall again that the classical theorist chooses the value $c$ in her test so that, regardless of what the null hypothesis $\\mu_0$ is, \n",
    "\n",
    "the probability that \" '$\\pm$ c of the average' contains $\\mu_0$\" is above $.95$. \n",
    "\n",
    "However, note that in the probability statement, $\\mu_0$ is fixed and not variable. \n",
    "\n",
    "The Bayesian has a way of seeing these confidence intervals as genuine statements about the probability of the parameter being in a certain interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The components of Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem takes the following form in the context of statistical inference:\n",
    "\n",
    "$$p(\\theta \\mid x) = \\frac{p(x\\mid \\theta)\\cdot p(\\theta)}{p(x)}$$\n",
    "\n",
    "which as per usual you remember in the mnemonic:\n",
    "\n",
    "$$\\mathrm{posterior} = \\frac{\\mathrm{likelihood}\\times \\mathrm{prior}}{\\mathrm{evidence}}$$\n",
    "\n",
    "Here is a compact summary of the components: \n",
    "\n",
    "| concept | notation | definition | how to find value |\n",
    "|----------|----------|----------|----------|\n",
    "| prior   | $p(\\theta)$   |  prior degree of belief that $X\\sim p_{\\theta}$  | initially, consult your degrees of belief, which might be recorded in frequency information which you accept; after the initial round, the prior is the previous round's posterior |\n",
    "| likelihood   | $p(x\\mid \\theta)$   | probability that $X=x$ conditional on $X\\sim p_{\\theta}$     | $p_{\\theta}(x)$, i.e. input in $x$ to pdf $p_{\\theta}$; look at graph or consult computer or book; if in applied context look at \"if $\\theta$ were ... then $x$ would be ...\" statements salient in the subject matter |\n",
    "| evidence (aka marginal, aka prior predictive distribution)  | $p(x)$  | probability that $X=x$  | use formula $p(x)=\\sum_{\\theta\\in \\Theta} p(\\theta)\\cdot p(x\\mid \\theta)$; in non-trivial cases use computer  |\n",
    "| posterior  | $p(\\theta \\mid x)$  | probability that $X\\sim p_{\\theta}$ conditional on $X=x$  | Use Bayes' Theorem and three previous rows  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Bayes' Theorem with uniform prior\n",
    "\n",
    "Here is a familiar example of Bayes' Theorem:\n",
    "\n",
    "1. The parameter space is small, with each pdf in it being represented by a solid line in the graph to the top left. In this case, the pdfs are some normals. \n",
    "\n",
    "2. We have a uniform prior represented by the pie chart to the bottom left.\n",
    "\n",
    "3. Using Bayes' Theorem we can calculate $p(\\theta\\mid x)$ for all values of $\\theta,x$: the different values of $\\theta$ correspond to the different solid lines on the top right, and the different values of $x$ are on the bottom right. \n",
    "\n",
    "4. If observe a value of $x$ and look at the values right above it-- where the colored lines intersect the black vertical line-- we get the posterior $p(\\theta\\mid x)$. These add up to one and we can represent them in the pie chart to the bottom left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# size_sample_space, size_parameter_space are positive integers\n",
    "# prior is list of length size_parameter_space of non-negative numbers that sum to 1\n",
    "# likelihood is a list of length size_parameter_space \n",
    "# where each element is a list of length size_sample_space numbers that sum to 1\n",
    "\n",
    "def bayes(size_sample_space,size_parameter_space,prior,likelihood, observed_value = None, round_digits = 4):\n",
    "\n",
    "    if not 0.98 <= sum(prior) <= 1.02:\n",
    "        raise ValueError(\"The sum of the prior probabilities must be equal to 1\")\n",
    "    if len(prior) != size_parameter_space:\n",
    "        raise ValueError(\"The length of the prior probabilities must be equal to the size of the parameter space\")\n",
    "    if len(likelihood) != size_parameter_space:\n",
    "        raise ValueError(\"The length of the likelihood must be equal to the size of the parameter space\")\n",
    "    if any([len(likelihood[i]) != size_sample_space for i in range(size_parameter_space)]):\n",
    "        raise ValueError(\"The length of each entry in the likelihood must be equal to the size of the sample space\")\n",
    "  \n",
    "\n",
    "    sample_space = list(range(size_sample_space))\n",
    "\n",
    "    parameter_space = list(range(size_parameter_space))\n",
    "\n",
    "    labels = ['θ_'+str(i) for i in range(size_parameter_space)]  \n",
    "\n",
    "    parameter_space_names = []\n",
    "\n",
    "    for i in parameter_space:\n",
    "        parameter_space_names.append(f\"likelihood p(x_j  &#124; θ_{i})\")\n",
    "\n",
    "    evidence =  []\n",
    "\n",
    "    for j in sample_space:\n",
    "        evidence.append(sum([prior[i]*likelihood[i][j] for i in parameter_space]))\n",
    "\n",
    "    sample_space_names = []\n",
    "\n",
    "    for j in sample_space:\n",
    "        sample_space_names.append(f\"x_{j}, evidence = {evidence[j].round(round_digits)}\")\n",
    "\n",
    "\n",
    "    posterior = [[None]*size_sample_space for _ in range(size_parameter_space)]\n",
    "\n",
    "    for i in parameter_space:\n",
    "        for j in sample_space:\n",
    "            posterior[i][j] = ((likelihood[i][j]*prior[i]) / evidence[j])\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 5))  # 2 row, 2 columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in parameter_space:\n",
    "        if i == 0:\n",
    "            axs[0,0].plot(sample_space, likelihood[i], label='θ_0')\n",
    "        else:\n",
    "            if i<size_parameter_space-1:\n",
    "                axs[0,0].plot(sample_space, likelihood[i])\n",
    "            else:\n",
    "                axs[0,0].plot(sample_space, likelihood[i], label='θ_%i' % i)\n",
    "    axs[0,0].plot(sample_space, evidence, '--', label='evidence', color = 'black')        \n",
    "    axs[0,0].set_ylim(0, max(max(likelihood[0]), max(likelihood[1]))+.001)\n",
    "    axs[0,0].set_title('likelihood p(x | θ)')\n",
    "    axs[0,0].set_xlabel('x')\n",
    "    axs[0,0].set_ylim(0, 1.1)\n",
    "    axs[0,0].legend(loc='upper right')    \n",
    "\n",
    "    for i in parameter_space:\n",
    "        if i == 0:\n",
    "            axs[0,1].plot(sample_space, posterior[i], label='θ_0')\n",
    "        else:\n",
    "            if i<size_parameter_space-1:\n",
    "                axs[0,1].plot(sample_space, posterior[i])\n",
    "            else:\n",
    "                axs[0,1].plot(sample_space, posterior[i], label='θ_%i' % i)\n",
    "    if observed_value is not None:\n",
    "        axs[0,1].axvline(x=observed_value, linestyle='--', color = 'black', label='observed value')       \n",
    "    axs[0,1].set_ylim(0, 1.1)\n",
    "    axs[0,1].set_title('posterior p(θ  | x)')\n",
    "    axs[0,1].set_xlabel('x')\n",
    "    axs[0,1].legend(loc='upper right') \n",
    "\n",
    "    fig.subplots_adjust(top=.8)\n",
    "\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "\n",
    "    fig.suptitle('insert')\n",
    "\n",
    "    sorted_prior_indices = np.argsort(prior)\n",
    "    above_02_indices_prior = [i for i in sorted_prior_indices if prior[i] > 0.04]\n",
    "    labels_prior = [labels[i] if i in above_02_indices_prior else '' for i in range(size_parameter_space)]\n",
    "\n",
    "    axs[1,0].pie(prior, labels=labels_prior)\n",
    "    axs[1,0].set_title('prior p(θ) uniform')\n",
    "\n",
    "\n",
    "\n",
    "    evidence_observed = sum([prior[i]*likelihood[i][observed_value] for i in range(size_parameter_space)])\n",
    "\n",
    "    posterior_observed = [prior[i]*likelihood[i][observed_value]/evidence_observed for i in range(size_parameter_space)]\n",
    "\n",
    "    sorted_posterior_indices = np.argsort(posterior_observed)\n",
    "    above_02_indices_posterior = [i for i in sorted_posterior_indices if posterior_observed[i] > 0.04]\n",
    "    labels_posterior = [labels[i] if i in above_02_indices_posterior else '' for i in range(size_parameter_space)]\n",
    "\n",
    "\n",
    "    axs[1,1].pie(posterior_observed, labels=labels_posterior)    \n",
    "    axs[1,1].set_title('posterior p(θ| %1.2f) non-uniform' % observed_value)\n",
    "\n",
    "    fig.subplots_adjust(top=.8)\n",
    "\n",
    "    fig.suptitle('Visualizing Bayes\\' Theorem with uniform prior')\n",
    "\n",
    "\n",
    "    return posterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def bayes_many_normals(size_sample_space,size_parameter_space, observed_value = None):\n",
    "\n",
    "    n = size_sample_space\n",
    "    \n",
    "    l = size_parameter_space\n",
    "    \n",
    "    my_prior = [1/l] * l\n",
    "\n",
    "    my_mean = [l//2 + .5*(l//2 - k) for k in range(l)]\n",
    "\n",
    "    my_var = [1] * l\n",
    "\n",
    "    my_likelihood = [None]*l    \n",
    "\n",
    "    std_dev = [None]*l\n",
    "\n",
    "    my_min= [None]*l\n",
    "\n",
    "    my_max= [None]*l\n",
    "\n",
    "    for i in range(l):\n",
    "        std_dev[i] = np.sqrt(my_var[i])\n",
    "\n",
    "    my_min = min([my_mean[i] - 3*std_dev[i] for i in range(l)])\n",
    "    my_max = max([my_mean[i] + 3*std_dev[i] for i in range(l)])\n",
    "\n",
    "    my_sample_space_alt = np.linspace(my_min, my_max, size_sample_space)\n",
    "\n",
    "    for i in range(l):\n",
    "        my_likelihood[i] = norm.pdf(my_sample_space_alt, my_mean[i], std_dev[i])\n",
    "\n",
    "\n",
    "    bayes(n,l,my_prior,my_likelihood, observed_value = observed_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a74888cb6154949813bdf7bdc6464ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=50, description='size_sample_space', min=1), IntSlider(value=4, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.bayes_many_normals(size_sample_space, size_parameter_space, observed_value=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(bayes_many_normals, \n",
    "         size_sample_space= IntSlider(min=1, max=100, step=1, value=50), \n",
    "         size_parameter_space= IntSlider(min=1, max=10, step=1, value=4),\n",
    "         observed_value = IntSlider(min=1, max=99, step=1, value=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Bayes' Theorem Better with uniform prior\n",
    "\n",
    "The pie charts are helpful for certain things-- one can e.g. quickly look at them and see how the posterior is non-uniform. \n",
    "\n",
    "We can also represent it as a histogram, i.e. by lining up $\\theta_0, \\theta_1, \\ldots$ on $x$-axis and by plotting the values of $p(\\theta_0), p(\\theta_1), \\ldots$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# size_sample_space, size_parameter_space are positive integers\n",
    "# prior is list of length size_parameter_space of non-negative numbers that sum to 1\n",
    "# likelihood is a list of length size_parameter_space \n",
    "# where each element is a list of length size_sample_space numbers that sum to 1\n",
    "\n",
    "def bayes_better(size_sample_space,size_parameter_space,prior,likelihood, observed_value = None, round_digits = 4):\n",
    "\n",
    "    if not 0.98 <= sum(prior) <= 1.02:\n",
    "        raise ValueError(\"The sum of the prior probabilities must be equal to 1\")\n",
    "    if len(prior) != size_parameter_space:\n",
    "        raise ValueError(\"The length of the prior probabilities must be equal to the size of the parameter space\")\n",
    "    if len(likelihood) != size_parameter_space:\n",
    "        raise ValueError(\"The length of the likelihood must be equal to the size of the parameter space\")\n",
    "    if any([len(likelihood[i]) != size_sample_space for i in range(size_parameter_space)]):\n",
    "        raise ValueError(\"The length of each entry in the likelihood must be equal to the size of the sample space\")\n",
    "  \n",
    "\n",
    "    sample_space = list(range(size_sample_space))\n",
    "\n",
    "    parameter_space = list(range(size_parameter_space))\n",
    "\n",
    "    labels = ['θ_'+str(i) for i in range(size_parameter_space)]  \n",
    "\n",
    "    parameter_space_names = []\n",
    "\n",
    "    for i in parameter_space:\n",
    "        parameter_space_names.append(f\"likelihood p(x_j  &#124; θ_{i})\")\n",
    "\n",
    "    evidence =  []\n",
    "\n",
    "    for j in sample_space:\n",
    "        evidence.append(sum([prior[i]*likelihood[i][j] for i in parameter_space]))\n",
    "\n",
    "    sample_space_names = []\n",
    "\n",
    "    for j in sample_space:\n",
    "        sample_space_names.append(f\"x_{j}, evidence = {evidence[j].round(round_digits)}\")\n",
    "\n",
    "\n",
    "    posterior = [[None]*size_sample_space for _ in range(size_parameter_space)]\n",
    "\n",
    "    for i in parameter_space:\n",
    "        for j in sample_space:\n",
    "            posterior[i][j] = ((likelihood[i][j]*prior[i]) / evidence[j])\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 5))  # 2 row, 2 columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in parameter_space:\n",
    "        if i == 0:\n",
    "            axs[0,0].plot(sample_space, likelihood[i], label='θ_0')\n",
    "        else:\n",
    "            if i<size_parameter_space-1:\n",
    "                axs[0,0].plot(sample_space, likelihood[i])\n",
    "            else:\n",
    "                axs[0,0].plot(sample_space, likelihood[i], label='θ_%i' % i)\n",
    "    axs[0,0].plot(sample_space, evidence, '--', label='evidence', color = 'black')        \n",
    "    axs[0,0].set_ylim(0, max(max(likelihood[0]), max(likelihood[1]))+.001)\n",
    "    axs[0,0].set_title('likelihood p(x | θ)')\n",
    "    axs[0,0].set_xlabel('x')\n",
    "    axs[0,0].set_ylim(0, 1.1)\n",
    "    axs[0,0].legend(loc='upper right')    \n",
    "\n",
    "    for i in parameter_space:\n",
    "        if i == 0:\n",
    "            axs[0,1].plot(sample_space, posterior[i], label='θ_0')\n",
    "        else:\n",
    "            if i<size_parameter_space-1:\n",
    "                axs[0,1].plot(sample_space, posterior[i])\n",
    "            else:\n",
    "                axs[0,1].plot(sample_space, posterior[i], label='θ_%i' % i)\n",
    "    if observed_value is not None:\n",
    "        axs[0,1].axvline(x=observed_value, linestyle='--', color = 'black', label='observed value')       \n",
    "    axs[0,1].set_ylim(0, 1.1)\n",
    "    axs[0,1].set_title('posterior p(θ  | x)')\n",
    "    axs[0,1].set_xlabel('x')\n",
    "    axs[0,1].legend(loc='upper right') \n",
    "\n",
    "    fig.subplots_adjust(top=.8)\n",
    "\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "\n",
    "    sorted_prior_indices = np.argsort(prior)\n",
    "    above_02_indices_prior = [i for i in sorted_prior_indices if prior[i] > 0.04]\n",
    "    labels_prior = [labels[i] if i in above_02_indices_prior else '' for i in range(size_parameter_space)]\n",
    "\n",
    "    axs[1,0].plot(parameter_space, prior, color='fuchsia')\n",
    "    axs[1,0].set_title('prior p(θ) uniform')\n",
    "    axs[1,0].set_xlabel('θ')    \n",
    "\n",
    "    evidence_observed = sum([prior[i]*likelihood[i][observed_value] for i in range(size_parameter_space)])\n",
    "\n",
    "    posterior_observed = [prior[i]*likelihood[i][observed_value]/evidence_observed for i in range(size_parameter_space)]\n",
    "\n",
    "    axs[1,1].plot(parameter_space, posterior_observed, color='fuchsia')\n",
    "    axs[1,1].set_title('posterior p(θ  | %1.2f) non-uniform' % observed_value)\n",
    "    axs[1,1].set_xlabel('θ')   \n",
    "\n",
    "    fig.subplots_adjust(top=.8)\n",
    "\n",
    "    fig.suptitle('Visualizing Bayes\\' Theorem Better with uniform prior')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def bayes_many_normals_better(size_sample_space,size_parameter_space, observed_value = None):\n",
    "\n",
    "    n = size_sample_space\n",
    "    \n",
    "    l = size_parameter_space\n",
    "    \n",
    "    my_prior = [1/l] * l\n",
    "\n",
    "    my_mean = [l//2 + .5*(l//2 - k) for k in range(l)]\n",
    "\n",
    "    my_var = [1] * l\n",
    "\n",
    "    my_likelihood = [None]*l    \n",
    "\n",
    "    std_dev = [None]*l\n",
    "\n",
    "    my_min= [None]*l\n",
    "\n",
    "    my_max= [None]*l\n",
    "\n",
    "    for i in range(l):\n",
    "        std_dev[i] = np.sqrt(my_var[i])\n",
    "\n",
    "    my_min = min([my_mean[i] - 3*std_dev[i] for i in range(l)])\n",
    "    my_max = max([my_mean[i] + 3*std_dev[i] for i in range(l)])\n",
    "\n",
    "    my_sample_space_alt = np.linspace(my_min, my_max, size_sample_space)\n",
    "\n",
    "    for i in range(l):\n",
    "        my_likelihood[i] = norm.pdf(my_sample_space_alt, my_mean[i], std_dev[i])\n",
    "\n",
    "\n",
    "    bayes_better(n,l,my_prior,my_likelihood, observed_value = observed_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee40ff0ccf24a1e95a10f1dd0d539b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='size_sample_space', min=1), IntSlider(value=15, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.bayes_many_normals_better(size_sample_space, size_parameter_space, observed_value=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(bayes_many_normals_better, \n",
    "         size_sample_space= IntSlider(min=1, max=100, step=1, value=100), \n",
    "         size_parameter_space= IntSlider(min=1, max=50, step=1, value=15),\n",
    "         observed_value = IntSlider(min=1, max=99, step=1, value=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When visualizing Bayes' Theorem with binomial prior, one sees that the posterior is also bell-shaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# size_sample_space, size_parameter_space are positive integers\n",
    "# prior is list of length size_parameter_space of non-negative numbers that sum to 1\n",
    "# likelihood is a list of length size_parameter_space \n",
    "# where each element is a list of length size_sample_space numbers that sum to 1\n",
    "\n",
    "def bayes_better_binom(size_sample_space,size_parameter_space,prior,likelihood, q, observed_value = None, round_digits = 4):\n",
    "\n",
    "    if not 0.98 <= sum(prior) <= 1.02:\n",
    "        raise ValueError(\"The sum of the prior probabilities must be equal to 1\")\n",
    "    if len(prior) != size_parameter_space:\n",
    "        raise ValueError(\"The length of the prior probabilities must be equal to the size of the parameter space\")\n",
    "    if len(likelihood) != size_parameter_space:\n",
    "        raise ValueError(\"The length of the likelihood must be equal to the size of the parameter space\")\n",
    "    if any([len(likelihood[i]) != size_sample_space for i in range(size_parameter_space)]):\n",
    "        raise ValueError(\"The length of each entry in the likelihood must be equal to the size of the sample space\")\n",
    "  \n",
    "\n",
    "    sample_space = list(range(size_sample_space))\n",
    "\n",
    "    parameter_space = list(range(size_parameter_space))\n",
    "\n",
    "    labels = ['θ_'+str(i) for i in range(size_parameter_space)]  \n",
    "\n",
    "    parameter_space_names = []\n",
    "\n",
    "    for i in parameter_space:\n",
    "        parameter_space_names.append(f\"likelihood p(x_j  &#124; θ_{i})\")\n",
    "\n",
    "    evidence =  []\n",
    "\n",
    "    for j in sample_space:\n",
    "        evidence.append(sum([prior[i]*likelihood[i][j] for i in parameter_space]))\n",
    "\n",
    "    sample_space_names = []\n",
    "\n",
    "    for j in sample_space:\n",
    "        sample_space_names.append(f\"x_{j}, evidence = {evidence[j].round(round_digits)}\")\n",
    "\n",
    "\n",
    "    posterior = [[None]*size_sample_space for _ in range(size_parameter_space)]\n",
    "\n",
    "    for i in parameter_space:\n",
    "        for j in sample_space:\n",
    "            posterior[i][j] = ((likelihood[i][j]*prior[i]) / evidence[j])\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 5))  # 2 row, 2 columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in parameter_space:\n",
    "        if i == 0:\n",
    "            axs[0,0].plot(sample_space, likelihood[i], label='θ_0')\n",
    "        else:\n",
    "            if i<size_parameter_space-1:\n",
    "                axs[0,0].plot(sample_space, likelihood[i])\n",
    "            else:\n",
    "                axs[0,0].plot(sample_space, likelihood[i], label='θ_%i' % i)\n",
    "    axs[0,0].plot(sample_space, evidence, '--', label='evidence', color = 'black')        \n",
    "    axs[0,0].set_ylim(0, max(max(likelihood[0]), max(likelihood[1]))+.001)\n",
    "    axs[0,0].set_title('likelihood p(x | θ)')\n",
    "    axs[0,0].set_xlabel('x')\n",
    "    axs[0,0].set_ylim(0, 1.1)\n",
    "    axs[0,0].legend(loc='upper right')    \n",
    "    \n",
    "    for i in parameter_space:\n",
    "        if i == 0:\n",
    "            axs[0,1].plot(sample_space, posterior[i], label='θ_0')\n",
    "        else:\n",
    "            if i<size_parameter_space-1:\n",
    "                axs[0,1].plot(sample_space, posterior[i])\n",
    "            else:\n",
    "                axs[0,1].plot(sample_space, posterior[i], label='θ_%i' % i)\n",
    "    if observed_value is not None:\n",
    "        axs[0,1].axvline(x=observed_value, linestyle='--', color = 'black', label='observed value')       \n",
    "    axs[0,1].set_ylim(0, 1.1)\n",
    "    axs[0,1].set_title('posterior p(θ  | x)')\n",
    "    axs[0,1].set_xlabel('x')\n",
    "    axs[0,1].legend(loc='upper right') \n",
    "\n",
    "    fig.subplots_adjust(top=.8)\n",
    "\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "\n",
    "    sorted_prior_indices = np.argsort(prior)\n",
    "    above_02_indices_prior = [i for i in sorted_prior_indices if prior[i] > 0.04]\n",
    "    labels_prior = [labels[i] if i in above_02_indices_prior else '' for i in range(size_parameter_space)]\n",
    "\n",
    "    axs[1,0].plot(parameter_space, prior, color='fuchsia')\n",
    "    axs[1,0].set_title('prior Binomial(%i,%1.2f)' % (size_parameter_space,q))\n",
    "    axs[1,0].set_xlabel('θ')    \n",
    "    axs[1,0].set_ylim(0, .4)    \n",
    "\n",
    "    evidence_observed = sum([prior[i]*likelihood[i][observed_value] for i in range(size_parameter_space)])\n",
    "\n",
    "    posterior_observed = [prior[i]*likelihood[i][observed_value]/evidence_observed for i in range(size_parameter_space)]\n",
    "\n",
    "    axs[1,1].plot(parameter_space, posterior_observed, color='fuchsia')\n",
    "    axs[1,1].set_title('posterior p(θ  | %1.2f), also near bell-shaped' % observed_value)\n",
    "    axs[1,1].set_xlabel('θ')   \n",
    "    axs[1,1].set_ylim(0, .4)        \n",
    "\n",
    "    fig.subplots_adjust(top=.8)\n",
    "\n",
    "    fig.suptitle('Visualizing Bayes\\' Theorem Better with binomial prior')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def bayes_many_normals_better_binom(size_sample_space,size_parameter_space, q, observed_value = None):\n",
    "\n",
    "    n = size_sample_space\n",
    "    \n",
    "    l = size_parameter_space\n",
    "    \n",
    "    my_prior = binom.pmf(range(l), l-1, q)\n",
    "\n",
    "    my_mean = [l//2 + .5*(l//2 - k) for k in range(l)]\n",
    "\n",
    "    my_var = [1] * l\n",
    "\n",
    "    my_likelihood = [None]*l    \n",
    "\n",
    "    std_dev = [None]*l\n",
    "\n",
    "    my_min= [None]*l\n",
    "\n",
    "    my_max= [None]*l\n",
    "\n",
    "    for i in range(l):\n",
    "        std_dev[i] = np.sqrt(my_var[i])\n",
    "\n",
    "    my_min = min([my_mean[i] - 3*std_dev[i] for i in range(l)])\n",
    "    my_max = max([my_mean[i] + 3*std_dev[i] for i in range(l)])\n",
    "\n",
    "    my_sample_space_alt = np.linspace(my_min, my_max, size_sample_space)\n",
    "\n",
    "    for i in range(l):\n",
    "        my_likelihood[i] = norm.pdf(my_sample_space_alt, my_mean[i], std_dev[i])\n",
    "\n",
    "\n",
    "    bayes_better_binom(n,l,my_prior,my_likelihood, q, observed_value = observed_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f6c8a0c1a64731a95a38aa80387518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='size_sample_space', min=1), IntSlider(value=40, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.bayes_many_normals_better_binom(size_sample_space, size_parameter_space, q, observed_value=None)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(bayes_many_normals_better_binom, \n",
    "         size_sample_space= IntSlider(min=1, max=100, step=1, value=100), \n",
    "         size_parameter_space= IntSlider(min=1, max=100, step=1, value=40),\n",
    "         q = FloatSlider(min=0, max=1, step=0.01, value=0.5),\n",
    "        observed_value = IntSlider(min=1, max=99, step=1, value=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normals with normal prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def posterior_parameters(σ2,a,b2,n,X̅):\n",
    "\n",
    "    σ = np.sqrt(σ2)\n",
    "\n",
    "    b = np.sqrt(b2)\n",
    "\n",
    "    se = σ/np.sqrt(n)\n",
    "\n",
    "    w = (1/se**2)/((1/se**2)+(1/b**2))\n",
    "\n",
    "    θ̅ = w*X̅ + (1-w)*a\n",
    "\n",
    "    τ = np.sqrt(1/(1/se**2 + (1/b**2)))\n",
    "\n",
    "    return θ̅, τ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed59ffcb3dd24a5f9ff0c1516ce9d733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='σ2', max=10.0, min=0.1), FloatSlider(value=0.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.posterior_parameters(σ2, a, b2, n, X̅)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(posterior_parameters, \n",
    "         σ2=FloatSlider(min=0.1, max=10, step=0.1, value=1), \n",
    "         a=FloatSlider(min=0, max=20, step=0.1, value=0), \n",
    "         b2=FloatSlider(min=0, max=100000000000, step=0.1, value=1000000), \n",
    "         n=FloatSlider(min=1, max=100, step=0.1, value=49), \n",
    "         X̅=FloatSlider(min=0, max=20, step=0.1, value=10.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def posterior_plot(σ2,a,b2,n,X̅,c,k):\n",
    "\n",
    "    σ = np.sqrt(σ2)\n",
    "\n",
    "    b = np.sqrt(b2)\n",
    "\n",
    "    y = np.zeros((k,1000))\n",
    "\n",
    "    θ = np.random.normal(a, b, k)\n",
    "\n",
    "    # Generate points on the x axis between -10 and 10:\n",
    "    x = np.linspace(-20, 20, 1000)\n",
    "\n",
    "    for i in range(k):\n",
    "        y[i] = norm.pdf(x, θ[i],σ)\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 2, figsize=(10, 5))  # 1 row, 1 columns\n",
    "\n",
    "    # Calculate the PDF of the normal distribution at these points:\n",
    "    for i in range(k):\n",
    "        axs[0,0].plot(x, y[i]) \n",
    "    axs[0,0].set_xlim(-5, 5)\n",
    "\n",
    "    axs[1,0].plot(x, norm.pdf(x, a, b))\n",
    "    axs[1,0].set_title('prior N(%1.2f,%1.2f)' % (a,b2))\n",
    "    axs[1,0].set_ylim(0, 1)        \n",
    "\n",
    "    fig.subplots_adjust(top=.8)\n",
    "\n",
    "    fig.subplots_adjust(hspace=1)\n",
    "\n",
    "\n",
    "    θ̅, τ = posterior_parameters(σ2,a,b2,n,X̅)\n",
    "\n",
    "    z = norm.pdf(x, θ̅, τ)\n",
    "    \n",
    "    axs[1,1].plot(x,z)\n",
    "    axs[1,1].set_xlim(9, 15)\n",
    "    axs[1,1].axvline(x=θ̅, color='b', linestyle='--', label='%1.2f' % θ̅)  # Add a vertical line at x=10\n",
    "    axs[1,1].set_title('posterior p(θ|X̅_%i)' % n)\n",
    "    axs[1,1].legend()  # Add the legend    \n",
    "\n",
    "    axs[2,1].plot(x,z)\n",
    "    axs[2,1].set_xlim(9, 15)\n",
    "    axs[2,1].set_title('posterior p(θ|X̅_%i)' % n)    \n",
    "\n",
    "\n",
    "    a = X̅-c\n",
    "    b = X̅+c\n",
    "\n",
    "    def f(x):\n",
    "        return norm.pdf(x, θ̅, τ)\n",
    "\n",
    "    integral, error = quad(f, a, b)\n",
    "\n",
    "    z1 = [z[i] if (a < x[i] and x[i] < b) else 0 for i in range(len(x))]\n",
    "    axs[2,1].plot(x,z1, alpha=0.1)\n",
    "    axs[2,1].fill_between(x, z1, color='green', alpha=0.25, label = 'Pr(X̅_%i-%1.2f<θ<X̅_%i+%1.2f) \\n = %1.2f' % (n,c, n,c,integral))  # Fill the area under the curve\n",
    "    axs[2,1].legend()  # Add the legend\n",
    "\n",
    "\n",
    "\n",
    "    # Set titles\n",
    "#    axs[0].set_title('likelihood p(x|θ_1),...,p(x|θ_%i) \\n θ_1,..., θ_%i ~N(%1.2f, %1.2f)' % (k, k, a, b2))\n",
    "#    axs[1].set_title('with X̅=%1.2f, \\n posterior p(θ|X̅)~N(%1.2f, %1.2f)' % (X̅, θ̅, τ))\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27a38848a744ddebe0192814844ec85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='σ2', max=10.0, min=0.1), FloatSlider(value=1.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.posterior_plot(σ2, a, b2, n, X̅, c, k)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(posterior_plot, \n",
    "    σ2=FloatSlider(min=0.1, max=10, step=0.1, value=1),\n",
    "    a=fixed(0),\n",
    "    b2=FloatSlider(min=1, max=200, step=1, value=1),\n",
    "    n=FloatSlider(min=1, max=100, step=0.1, value=49),\n",
    "    X̅=fixed(10.2),\n",
    "    c=FloatSlider(min=0, max=1, step=0.01, value=.28),    \n",
    "    k=IntSlider(min=1, max=100, step =1, value = 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build the following table, where variance and $n$ are choosen, and where the variance of the prior $b^2$ is really high (around 200), and then we search for the $c$ which makes $Pr(\\overline{X}_n-c<\\theta<\\overline{X}_n+c)=.95$.\n",
    "\n",
    "|  | variance ($\\sigma^2$) | $n$ | $c$ |\n",
    "|----------|----------|----------|----------|\n",
    "| Example 1   | 1   | 49   | .28   |\n",
    "| Example 2  |  1  | 100   | .20   |\n",
    "| Example 3  |  4 | 100  | .40  |\n",
    "| Example 4  | 4  | 49  | .56  |\n",
    "\n",
    "What we see is that we get the same table back as we previously had in the classical case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kass and Mayo on this Bayesian inference\n",
    "\n",
    "Kass {cite}`Kass2011-yj` describes how the Bayesian school would view this inference:\n",
    "\n",
    "> *Bayesian interpretation of the posterior interval.* Under the assumptions above, the probability that $\\theta$ is in the interval $I=(9.92, 10.48)$ [coming from (10.2-.28, 10.2+.28)$] is 0.95 ( {cite}`Kass2011-yj` p. 4).\n",
    "\n",
    "He contrasts this to his pragmatist view:\n",
    "\n",
    "> *Pragmatic interpretation of the posterior interval*. If the data were a random sample for which $\\overline{X}_n = \\overline{x}$ holds, that is, [we observe that $\\overline{X}_n = \\overline{x}=10.2$ holds], and if the assumptions above were to hold, then the probability that $\\theta$ is in the interval $I$ would be 0.95. This refers to a hypothetical value $\\overline{x}$ of the random variable $\\overline{X}_n$, and because $\\overline{X}_n$ lives in the theoretical world the statement remains theoretical. Nonetheless, we are able to draw useful conclusions from the data as long as our theoretical world is aligned well with the real world that produced the data ( {cite}`Kass2011-yj` p. 4)\n",
    "\n",
    "Mayo critizes Kass' pragmatist approach as follows:\n",
    "\n",
    "> I doubt that the Bayesian would be satisfied with life as a subjunctivist either. The payoff for the extra complexity of positing a prior is the ability to detach that the probability that $\\theta$ is in $(10.2-.29, 10.2+.28)$ is 0.95. [Further] We can hear some Bayesians tribe members grumbling at the very assumption that they're modeling the 'the real world that produced the data' ({cite}`Mayo2018-qa` p. 427)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}