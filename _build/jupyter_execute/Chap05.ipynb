{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminders about random variables and probability measures\n",
    "\n",
    "Recall that *a random variable* is just a function $X:\\Omega\\rightarrow \\mathbb{R}$.\n",
    "\n",
    "Recall that a *probability measure* is just a function $P$ from events to numbers inbetween zero and one which satisfies [the probability axioms](https://logic-teaching.github.io/philstatsbook/Chap02.html#probability-axioms). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pdf, cdf, and ccdf say how probable, in the sense of $P$, it is for the random variable $X$ to take certain values. \n",
    "\n",
    "- The *pdf* is the function $f(x)=P(X=x)$, which answers the question \"how probable is it for $X$ to have outcome $x$?\"\n",
    "\n",
    "- The *cdf* is the function $F(x)=P(X\\leq x)$, which answers the question \"how probable is it for $X$ to have outcome $\\leq x$\"?\n",
    "\n",
    "- The *ccdf* is the function $\\overline{F}(x)=P(X> x)$, which answers the question \"how probable is it for $X$ to have outcome $> x$\"?\n",
    "\n",
    "Again, there are subtleties with the pdf when the sample space is infinite; but we are going to put those to the side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability distributions\n",
    "\n",
    "The pdf, cdf, and ccdf determine one another, and so we just refer to them all collectively as the *probability distribution* of $X$.\n",
    "\n",
    "We write $X\\sim F$ to say that $X$ has probability distribution with cdf $F$. \n",
    "\n",
    "Similarly one sees $X\\sim f$ to say that $X$ has probability distribution with pdf $f$.\n",
    "\n",
    "Again when you see $X\\sim F$, the $X$ is a random variable, and the $F$ dictates how probable it is tha the random variable takes certain values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The task of statistical inference\n",
    "\n",
    "The task of statistical inference is to infer, based on data, what the probability distribution is, which is generating the data. \n",
    "\n",
    "E.g. am I dealing with a fair coin or a biased coin? Am I dealing with a bell shaped distribution or a distribution which is weighted towards the extremes?\n",
    "\n",
    "Hence the inference is \"*from* data to the nature of the *probability distribution* generating the data.\"\n",
    "\n",
    "Note the apparent contrast in character to traditional confirmation theory Ã  la [Bayes' Theorem](https://logic-teaching.github.io/philstatsbook/Chap02.html#theorem-bayes-theorem-formula), where the inference is \"from *the prior probability measure* and the evidence to the posterior probability measure.\" (Later when we look at Bayesian approaches in statistics we can ask whether this apparent contrast can be sustained: ostensibly what is missing so far is a prior)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The format of the probability distributions\n",
    "\n",
    "There are theoretically a ton of different probability distributions to choose from. But practically speaking, one just restricts attention to a few families of them, which are parameterized by a few simple numbers.\n",
    "\n",
    "For instance: \n",
    "\n",
    "- Binomial $\\mathrm{Binom}(n,p)$ is parameterized by the number of trials $n$ and the probability of success $p$ on each trial. \n",
    "\n",
    "- Poisson $\\mathrm{Pois}(\\lambda)$ is parameterized by the the rate $\\lambda$.\n",
    "\n",
    "The various numbers like $n,p$ and $\\lambda$ are called *the parameters*. In *parametric statistical inference* one just decides on a family ahead of time and then tries to use the data to figure out which parameter is at issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we assume that we are dealing with a family $\\{F_{\\theta}: \\theta\\in \\Theta\\}$ of probability distributions, which come from one of the families, and we are trying to figure out which one of these is generating our data. \n",
    "\n",
    "The set $\\Theta$ is called *the parameter space*.\n",
    "\n",
    "For instance: \n",
    "\n",
    "- In one case we might be concerned with $\\{\\mathrm{Binomial}(n,p): 5\\leq n\\leq 10, .4\\leq p\\leq .6\\}$. In this case, the parameter space is $\\Theta = \\{5,\\ldots, 10\\}\\times [.4, ,.6]$, where $[.4,.6]$ is just all the real numbers inbetween and including .4 and .6 \n",
    "\n",
    "- In another case we might be concerned with $\\{\\mathrm{Pois}(\\lambda): 3\\leq \\lambda\\leq 10\\}$. In this case, the parameter space is $\\Theta = [3,10]$, where $[3,10]$ is all the real numbers inbetween and including 3 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The format of the data\n",
    "\n",
    "We assume that our data comes in the following format: \n",
    "\n",
    "a sequence $X_1, \\ldots, X_n\\sim F_{\\theta}$, \n",
    "\n",
    "where $\\theta$ is the true parameter of the distribution, \n",
    "\n",
    "and where $X_1, \\ldots, X_n$ are [independent](https://logic-teaching.github.io/philstatsbook/Chap04.html#independence-of-random-variables).\n",
    "\n",
    "We don't know what the parameter $\\theta$ is. We're trying to figure that out.\n",
    "\n",
    "Since we're in a probabilistic setting, we won't get exact knowledge of $\\theta$, but we'll get close, in various ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task of statistical inference\n",
    "\n",
    "We have a family of $\\{F_{\\theta}: \\theta\\in \\Theta\\}$ of probability distributions. \n",
    "\n",
    "One of them $F_{\\theta}$, whose parameter is called the *true parameter* $\\theta$, is generating the data.\n",
    "\n",
    "The data has the form of independent random variables $X_1, \\ldots, X_n \\sim F_{\\theta}$, where $\\theta$ is the true parameter. \n",
    "\n",
    "Our job is to look at the data and to try to figure out the true parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasserman writes: \n",
    "\n",
    "> Many inferential problems can be identified as being one of three types: estimation, confidence sets, and hypothesis testing ({cite}`Wasserman2013-bc` p. 90)\n",
    "\n",
    "Most philosophical discussion concerns hypothesis testing.\n",
    "\n",
    "In this chapter and the next we say a little about estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averages\n",
    "\n",
    "If one has some data $X_1, \\ldots, X_n$, the natural thing to do is to take its average $\\overline{X}_n$:\n",
    "\n",
    "$$\\overline{X}_n(\\omega) = \\frac{1}{n} \\sum_{i=1}^n X_i(\\omega) = \\frac{X_1(\\omega)+\\cdots +X_n(\\omega)}{n}$$\n",
    "\n",
    "If $n$ is clear from context, we may write $\\overline{X}$ instead of $\\overline{X}_n$.\n",
    "\n",
    "Hence, note the notational convention: an overline usually denotes an average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stigler {cite}`Stigler2016-lx` notes that the naturalness of this is a little puzzling, for two reasons:\n",
    "\n",
    "- On first principles: it is apriori not obvious how \"you can actually gain information by throwing information away\" and indeed this was a \"truly revolutionary\" idea ({cite}`Stigler2016-lx` pp. 3-4).\n",
    "\n",
    "- On the basis of the historical record: \"In antiquity, and in the Middle Ages, when reaching for a summary of diverse data, people chose an individual example\" and not an average (pp. 32-33). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use some simple ideas developed last time to try to dispel these puzzles.\n",
    "\n",
    "The first pertains to what happens in the limit.\n",
    "\n",
    "The second pertains to the risk at finite stages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens to averages in the limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposition (expectaton of the average)\n",
    "\n",
    "Suppose independent $X_1, \\ldots, X_n, \\ldots \\sim F_{\\theta}$ with common expectation $\\theta$.\n",
    "\n",
    "Then $\\mathbb{E}\\overline{X}_n = \\theta$.\n",
    "\n",
    "*Proof*:\n",
    "\n",
    "$\\mathbb{E}\\overline{X}_n = \\mathbb{E} \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}[X_i] = \\frac{1}{n} \\sum_{i=1}^n \\theta = \\frac{1}{n} \\cdot n \\cdot \\theta = \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next proposition was mentioned in [the last chapter](https://logic-teaching.github.io/philstatsbook/Chap04.html#proposition-root-n-rule):\n",
    "\n",
    "### Proposition (variance of the average; aka the root n rule)\n",
    "\n",
    "Suppose independent $X_1, \\ldots, X_n, \\ldots \\sim F_{\\theta}$ with common expectation $\\theta$ and variance $\\sigma^2$\n",
    "\n",
    "Then $\\mathrm{Var}(\\overline{X}_n) = \\frac{\\sigma^2}{n}$.\n",
    "\n",
    "*Proof*\n",
    "\n",
    "One has \n",
    "\n",
    "$\\mathrm{Var}(\\overline{X}_n) = \\sum_{i=1}^n \\mathrm{Var}(\\frac{X_i}{n})$ by independence\n",
    "\n",
    "$\\hspace{5mm} = \\sum_{i=1} \\frac{1}{n^2}  \\cdot \\mathrm{Var}(X_i)$ by [earlier proposition](https://logic-teaching.github.io/philstatsbook/Chap04.html#proposition-how-multiplication-and-addition-of-reals-affects-variance)\n",
    "\n",
    "$\\hspace{5mm} = \\frac{1}{n^2} \\sum_{i=1}  \\mathrm{Var}(X_i)$\n",
    "\n",
    "$\\hspace{5mm} = \\frac{1}{n^2} \\cdot n \\cdot \\sigma^2 = \\frac{\\sigma^2}{n}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency in mse of the average\n",
    "\n",
    "Suppose independent $X_1, \\ldots, X_n, \\ldots \\sim F_{\\theta}$ with common expectation $\\theta$ and variance $\\sigma^2$.\n",
    "\n",
    "Consider the following *the mean squared error of the average*, which one can think of as the distance between the average and the expectation:\n",
    "\n",
    "$$\\mathbb{E}(\\overline{X}_n-\\theta)^2$$\n",
    "\n",
    "Since $\\overline{X}_n$ itself has expectation $\\theta$, this is also $\\mathrm{Var}(\\overline{X}_n)$,\n",
    "\n",
    "which by the previous proposition is $\\mathrm{Var}(\\overline{X}_n) = \\frac{\\sigma^2}{n}$.\n",
    "\n",
    "Hence, as $n$ gets bigger and bigger, this goes to zero. \n",
    "\n",
    "That is, in the limit, the mean square error of the average goes to zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk in the finite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, for a random variable $Y$, one can consider the *mean squared error of the average*\n",
    "\n",
    "$$\\mathbb{E}(Y-\\theta)^2$$\n",
    "\n",
    "Consider again Stigler's remark:\n",
    "\n",
    "> \"In antiquity, and in the Middle Ages, when reaching for a summary of diverse data, people chose an individual example\" and not an average (pp. 32-33). \n",
    "\n",
    "Suppose that instead of the average, you choose an individual, like $X_{7}$.\n",
    "\n",
    "But one has that its mean squared error is \n",
    "\n",
    "$$\\mathbb{E}(X_7-\\theta)^2=\\mathrm{Var}(X_7)=\\sigma^2$$\n",
    "\n",
    "Hence, for any $n>1$ one has that \n",
    "\n",
    "$$\\mathbb{E}(\\overline{X}_n-\\theta)^2 = \\frac{\\sigma^2}{n}<\\sigma^2 =\\mathbb{E}(X_7-\\theta)^2$$\n",
    "\n",
    "Hence, in the short run, the average is less risky. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}