{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and simplicity\n",
    "\n",
    "The task of statistical inference, as we have pursued it here for the most part, is parameteric: it presupposes that one has already selected a family of parameters (binomials, normals, poisson, etc) at the beginning of the inquiry. But how does one know that this choice is right? This is the task of model selection. We go over one facet of it in this chapter, namely AIC and BIC.\n",
    "\n",
    "Simplicity is recognized as a component of theory choice, but it is often given a pragmatic or even aesthetic dimension. Sober noted that AIC and BIC can be viewed as fashinoning a systematic way to work out the tradeoff between simplicity and likelihood. Further, while it might not wear it on its sleeves, there are considerations related to Kullback-Lieber divergence which recommend or suggest a more rational or epistemic aspect to AIC and BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining binomial data example\n",
    "\n",
    "I am doing a trial of a new social intervention in one country, and you are doing it in another. \n",
    "\n",
    "- I had $n$ participants and model the number $X$ of successes in my population with $Binom(n, \\theta)$ with pdf $p_{\\theta}$ where $\\theta$ is unknown (we are using data to learn about it)\n",
    "- You had $m$ participants and model the successes $Y$ in your population with $Binom(m, \\theta^{\\prime})$ with pdf $p_{\\theta^{\\prime}}$ where $\\theta^{\\prime}$ is unknown (we are using data to learn about it). \n",
    "\n",
    "How should we model the combination of our two populations? \n",
    "\n",
    "- *One*: Should we model it as $X+Y\\sim Binom(n+m, \\theta^{\\prime\\prime})$ for some unknown $\\theta^{\\prime\\prime}$?\n",
    "- *Two*: Or at the other extreme should we assume that $X,Y$ are independent of one another and model the \"joint distribution\" $P(X=x \\wedge Y=y)=P(X=x)\\cdot P(Y=y)=p_{\\theta}(x)\\cdot p_{\\theta^{\\prime}}(y)$?\n",
    "\n",
    "These names *One* and *Two* are given because *Two* has one more parameter than *One*.\n",
    "\n",
    "(This example is a binomial version of Sober's normal example invovling farms from {cite}`Sober2015-of` pp. 129 ff. It is also covered in the case $k=2$ and $k=4$ of the discussion in Konishi-Kitagawa {cite}`Konishi2024-gk` pp. 75 ff of \"Checking the Equality of Two Discrete Distributions\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve fitting with normals example\n",
    "\n",
    "I have a lot of data points $(x_1, y_1), \\ldots, (x_n,y_n)$ and I am thinking that the $y$-variable in my population is some elementary function of the $x$-variable in my population, modulo some random 'noise factor' which I will model with $N(0,\\sigma)$. Hence, I am thinking that the model is going to be one of the following:\n",
    "\n",
    "- *Linear*: $Y = b_1\\cdot X+b_0+ \\epsilon$, where $\\epsilon\\sim N(0, \\sigma)$.\n",
    "\n",
    "- *Quadratic*: $Y = b_2\\cdot X^2+b_1\\cdot X+ b_0+\\epsilon$, where $\\epsilon\\sim N(0, \\sigma)$.\n",
    "\n",
    "- *Cubic*: $Y= b_3\\cdot X^3+ b_2\\cdot X^2+b_1\\cdot X+b_0+ \\epsilon$, where $\\epsilon\\sim N(0, \\sigma)$. \n",
    "\n",
    "Note that *Quaratic* has one more parameter than *Linear*, and *Cubic* has one more parameter than *Quadratic*.\n",
    "\n",
    "(For discussion of examples like this, see Haslwanter {cite}`Haslwanter2022-ak` pp. 238 ff)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC and BIC\n",
    "\n",
    "### AIC \n",
    "\n",
    "This is due to Akaike {cite}`Akaike1974-dx`, after whom it is named. The AIC of a model is defined by:\n",
    "\n",
    "$$ AIC = 2k - 2\\cdot log(p(x\\mid \\theta_{max}))$$\n",
    "\n",
    "where $k$ is the dimension of the parameters (how many real numbers you need to define it); and $p_{\\theta} = p(\\cdot \\mid \\theta)$ is the pdf of the family as a function of the choice of parameters; and $\\theta_{max}$ is where the likelihood $p(x\\mid \\theta)$ takes its maximum once $x$ is fixed.\n",
    "\n",
    "The way this is used it: lower AIC scores are better.\n",
    "\n",
    "Often one will see $\\theta_{max}$ abbreviated as $\\widehat{\\theta}$; note that this depends on $x$, but when people are writing it they just assume $x$ the exact observed value is fixed.\n",
    "\n",
    "Note that Sober does not include the constant 2, and he reverses the subtraction, so that on his way of talking higher AIC scores are better.\n",
    "\n",
    "### BIC \n",
    "\n",
    "The companion notion of BIC was defined by Schwarz {cite}`Schwarz1978-ja`. The BIC of a model is defined by:\n",
    "\n",
    "$$ BIC = k\\cdot \\ln(n) - 2\\cdot log(p(x\\mid \\theta_{max}))$$\n",
    "\n",
    "where $n$ is the number of observations, and where everything else is the same as in AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting the combining binomial data example\n",
    "\n",
    "We can graph the AIC and BIC in the 'combining binominal example' as follows.\n",
    "\n",
    "(For the correctneess of the equations we use here, see Konishi-Kitagawa {cite}`Konishi2024-gk` pp. 75 ff of \"Checking the Equality of Two Discrete Distributions\". For discussion of the correctness, see Konishi-Kitagawa {cite}`Konishi2024-gk` pp. 84 ff of \"Variable Selection for Regression Model\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.special import comb\n",
    "import matplotlib.patches as mpatches\n",
    "import metakernel\n",
    "import ipywidgets as widgets    \n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, FloatSlider, IntSlider\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def binom_aic_bic_visual(n,m):\n",
    "\n",
    "    X = np.arange(0, n)\n",
    "    Y = np.arange(0, m)\n",
    "    x,y = np.meshgrid(X, Y)\n",
    "\n",
    "    e = 1e-10  # small constant\n",
    "\n",
    "    log_one = np.log(comb(n+m, x+y) + e)-(x+y)*np.log((x+y)/(n+m) + e)-((n+m)-(x+y))*np.log((n+m-(x+y))/(n+m) + e)\n",
    "    log_two = np.log(comb(n, x) + e)-x*np.log(x/n + e)-(n-x+y)*np.log((n-x)/n + e)+ np.log(comb(m, y) + e)-y*np.log(y/m + e)-(m-y+x)*np.log((m-y)/m + e)\n",
    "    \n",
    "    aic_one = -2*(log_one-3)\n",
    "    aic_two = -2*(log_two-4)\n",
    "    bic_one = -(2*log_one-1*np.log(n+m))\n",
    "    bic_two = -(2*log_two-2*np.log(n+m))\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Create 3D subplots\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "    # Plot log_one and log_two on the left\n",
    "    ax1.plot_surface(x, y, log_one, cmap='viridis', label='log MLE_one')\n",
    "    ax1.plot_surface(x, y, log_two, cmap='plasma', label='log MLE_two')\n",
    "    ax1.set_title('log MLE_one vs. log MLE_two')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.legend()\n",
    "\n",
    "\n",
    "    # Plot aic_one and aic_two on the right\n",
    "    ax2.plot_surface(x, y, aic_one, cmap='viridis', label='AIC_one')\n",
    "    ax2.plot_surface(x, y, aic_two, cmap='plasma', label='AIC_two')\n",
    "    ax2.set_title('AIC_one vs. AIC_two')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Plot aic_one and aic_two on the right\n",
    "    ax3.plot_surface(x, y, bic_one, cmap='viridis', label='BIC_one')\n",
    "    ax3.plot_surface(x, y, bic_two, cmap='plasma', label='BIC_two')\n",
    "    ax3.set_title('BIC_one vs. BIC_two')\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    ax3.legend()\n",
    "\n",
    "    fig.suptitle('Two ways of combining Binom(%i,θ) and Binom(%i,θ\\')' % (n,m), fontsize=16)\n",
    "    fig.subplots_adjust(top=.2)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135d6f7c272a44e79b11f0ed6ccc1737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=250, description='n', max=500, min=1), IntSlider(value=250, description=…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactive(binom_aic_bic_visual,  n=(1, 500), m=(1, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def binom_aic_bic_visual_compare(n,m):\n",
    "\n",
    "    X = np.arange(0, n)\n",
    "    Y = np.arange(0, m)\n",
    "    x,y = np.meshgrid(X, Y)\n",
    " \n",
    "    e = 1e-5  # small constant\n",
    "\n",
    "    log_one = np.log(comb(n+m, x+y) + e)-(x+y)*np.log((x+y)/(n+m) + e)-((n+m)-(x+y))*np.log((n+m-(x+y))/(n+m) + e)\n",
    "    log_two = np.log(comb(n, x) + e)-x*np.log(x/n + e)-(n-x+y)*np.log((n-x)/n + e)+ np.log(comb(m, y) + e)-y*np.log(y/m + e)-(m-y+x)*np.log((m-y)/m + e)\n",
    "    log_compare = log_two - log_one\n",
    "\n",
    "    aic_one = -2*(log_one-3)\n",
    "    aic_two = -2*(log_two-4)\n",
    "    aic_compare = aic_two - aic_one\n",
    "\n",
    "    bic_one = -(2*log_one-1*np.log(n+m))\n",
    "    bic_two = -(2*log_two-2*np.log(n+m))\n",
    "    bic_compare = bic_two - bic_one\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Create 3D subplots\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "    # Plot log_one and log_two on the left\n",
    "    ax1.plot_surface(x, y, log_compare, cmap='coolwarm')\n",
    "    ax1.set_title('log MLE_two - log MLE_one')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot aic_one and aic_two on the right\n",
    "    ax2.plot_surface(x, y, aic_compare, cmap='coolwarm')\n",
    "    ax2.set_title('AIC_two - AIC_one')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot aic_one and aic_two on the right\n",
    "    ax3.plot_surface(x, y, bic_compare, cmap='coolwarm')\n",
    "    ax3.set_title('BIC_two - BIC_one')\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "\n",
    "\n",
    "    fig.suptitle('Two ways of combining Binom(%i,θ) and Binom(%i,θ\\')' % (n,m), fontsize=16)\n",
    "    fig.subplots_adjust(top=.2)\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9701ceba2440430b93a2b05248dcf1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=250, description='n', max=500, min=1), IntSlider(value=250, description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.binom_aic_bic_visual_compare(n, m)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(binom_aic_bic_visual_compare, n=(1, 500), m=(1, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting the curve fitting with normals example\n",
    "\n",
    "We can visual the curve-fitting with normal example as follows. \n",
    "\n",
    "(To generate these images, we use the python code from Haslwanter {cite}`Haslwanter2022-ak` pp. 238 ff))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def curve_fitting(n,b_0,b_1,b_2, c):\n",
    "\n",
    "    x = np.arange(n)\n",
    "    y = b_0+b_1*x +b_2*x**2 + c*np.random.randn(len(x))\n",
    "\n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    M1 = np.column_stack((np.ones_like(x), x))\n",
    "    M2 = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    M3 = np.column_stack((np.ones_like(x), x, x**2, x**3))\n",
    "\n",
    "    p1 = np.linalg.lstsq(M1, y, rcond=None)\n",
    "    p2 = np.linalg.lstsq(M2, y, rcond=None)\n",
    "    p3 = np.linalg.lstsq(M3, y, rcond=None)\n",
    "\n",
    "    y1 = p1[0][0] + p1[0][1]*x\n",
    "    y2 = p2[0][0] + p2[0][1]*x + p2[0][2]*x**2\n",
    "    y3 = p3[0][0] + p3[0][1]*x + p3[0][2]*x**2 + p3[0][3]*x**3\n",
    "\n",
    "    model1 = sm.OLS(y, M1)\n",
    "    results1 = model1.fit()\n",
    "\n",
    "    model2 = sm.OLS(y, M2)\n",
    "    results2 = model2.fit()\n",
    "\n",
    "    model3 = sm.OLS(y, M3)\n",
    "    results3 = model3.fit()\n",
    "\n",
    "\n",
    "\n",
    "    plt.scatter(x, y, s = 5)\n",
    "    plt.plot(x, y1, label='linear', alpha = .5, color = 'red')\n",
    "    plt.plot(x, y2, label='quadratic', alpha = .5, color = 'green')\n",
    "    plt.plot(x, y3, label='cubic', alpha = .5, color = 'orange')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Scatter plot of y vs x')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Coefficients from the linear fit: {p1[0]}')\n",
    "\n",
    "    print(f'Coefficients from the quadratic fit: {p2[0]}')\n",
    "\n",
    "    print(f'Coefficients from the cubic fit: {p2[0]}')\n",
    "\n",
    "    print(f'The AIC value is {results1.aic:4.1f} for the linear fit. \\n' + f'The AIC value is {results2.aic:4.1f} for the quadratic fit. \\n' + f'The AIC value is {results3.aic:4.1f} for the cubic fit.')\n",
    "\n",
    "    print(f'The BIC value is {results1.bic:4.1f} for the linear fit. \\n' + f'The BIC value is {results2.bic:4.1f} for the quadratic fit. \\n' + f'The BIC value is {results3.bic:4.1f} for the cubic fit.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47513638f5e540fdb6f37d2a3cb5b9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=100, description='n', min=1), FloatSlider(value=10.0, description='b_0',…"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactive(curve_fitting\n",
    "            , n=IntSlider(min=1, max=100, step=1, value=100)\n",
    "            , b_0=FloatSlider(min=0, max=10, step=.1, value=150)\n",
    "            , b_1=FloatSlider(min=0, max=10, step=.1, value=3)\n",
    "            , b_2=FloatSlider(min=0, max=10, step=.01, value=.03)\n",
    "            , c=FloatSlider(min=0, max=10, step=.1, value=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The epistemic import of likelihood\n",
    "\n",
    "\n",
    "### Recall Kullback-Liebler divergence\n",
    "\n",
    "Recall that we defined the Kullback-Leibler divergence as follows\n",
    "\n",
    "$d_{KL}(p,q) = \\sum_x p(x)\\cdot \\log \\frac{p(x)}{q(x)}$ \n",
    "\n",
    "and we think about this as a kind of generalized distance between $p$ and $q$. \n",
    "\n",
    "### Choices with higher expected likelihoods are closer to the truth\n",
    "\n",
    "The following proposition is part of the story of why a good AIC score is epistemic in nature: high (expected) likelihoods are closer to the truth in the sense of Kullback-Leibler divergence. There is admittedly more to the story about how to derive AIC from considerations of divergence. But it is why Sober calls a good AIC score a mark of \"predictive accuracy\" ({cite}`Sober2015-of` p. 131) and it is why he and Forster think that it is a positive reason to endorse the likelihood approach ({cite}`Forster2010-on` pp. 161-162). \n",
    "\n",
    "*Proposition*\n",
    "\n",
    "Suppose that $p_0$ is the pdf which gives the true distribution of our random variable. \n",
    "\n",
    "For any other parameters $\\theta, \\theta^{\\prime}$ the following are equivalent:\n",
    "\n",
    "- $d_{KL}(p_0, p_{\\theta})< d_{KL}(p_0, p_{\\theta^{\\prime}})$\n",
    "\n",
    "- $\\mathbb{E}_0 \\log p_{\\theta} <\\mathbb{E}_0 \\log p_{\\theta^{\\prime}}$\n",
    "\n",
    "where the expectations in the second are with respect to $p_{0}$.\n",
    "\n",
    "*Proof*:\n",
    "\n",
    "One has $d_{KL}(p_0, p_{\\theta})< d_{KL}(p_0, p_{\\theta^{\\prime}})$\n",
    "\n",
    "iff $\\sum_x p_0(x)\\cdot \\log \\frac{p_0(x)}{p_{\\theta}(x)}< \\sum_x p_0(x)\\cdot \\log \\frac{p_0(x)}{p_{\\theta^{\\prime}}(x)}$\n",
    "\n",
    "iff $\\sum_x p_0(x)\\cdot \\log p_0(x) - \\sum_x p_0(x)\\cdot \\log p_{\\theta}(x)< \\sum_x p_0(x)\\cdot \\log p_0(x) - \\sum_x p_0(x)\\cdot \\log p_{\\theta^{\\prime}}(x)$\n",
    "\n",
    "iff $- \\sum_x p_0(x)\\cdot \\log p_{\\theta}(x) < - \\sum_x p_0(x)\\cdot \\log p_{\\theta^{\\prime}}(x)$\n",
    "\n",
    "iff $\\sum_x p_0(x)\\cdot \\log p_{\\theta^{\\prime}}(x) < \\sum_x p_0(x)\\cdot \\log p_{\\theta}(x)$\n",
    "\n",
    "iff $\\mathbb{E}_0 \\log p_{\\theta} <\\mathbb{E}_0 \\log p_{\\theta^{\\prime}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
