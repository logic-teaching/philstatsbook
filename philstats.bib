%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Sean Walsh at 2024-04-15 20:23:16 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@book{Stigler2016-lx,
	abstract = {What gives statistics its unity as a science? Stephen Stigler
               sets forth the seven foundational ideas of statistics---a
               scientific discipline related to but distinct from mathematics
               and computer science.Even the most basic idea---aggregation,
               exemplified by averaging---is counterintuitive. It allows one to
               gain information by discarding information, namely, the
               individuality of the observations. Stigler's second pillar,
               information measurement, challenges the importance of ``big
               data'' by noting that observations are not all equally
               important: the amount of information in a data set is often
               proportional to only the square root of the number of
               observations, not the absolute number. The third idea is
               likelihood, the calibration of inferences with the use of
               probability. Intercomparison is the principle that statistical
               comparisons do not need to be made with respect to an external
               standard. The fifth pillar is regression, both a paradox (tall
               parents on average produce shorter children; tall children on
               average have shorter parents) and the basis of inference,
               including Bayesian inference and causal reasoning. The sixth
               concept captures the importance of experimental design---for
               example, by recognizing the gains to be had from a combinatorial
               approach with rigorous randomization. The seventh idea is the
               residual: the notion that a complicated phenomenon can be
               simplified by subtracting the effect of known causes, leaving a
               residual phenomenon that can be explained more easily.The Seven
               Pillars of Statistical Wisdom presents an original, unified
               account of statistical science that will fascinate the
               interested layperson and engage the professional statistician.},
	author = {Stigler, Stephen M},
	date-added = {2024-04-15 20:23:15 -0700},
	date-modified = {2024-04-15 20:23:15 -0700},
	language = {en},
	month = mar,
	publisher = {Harvard University Press},
	title = {The Seven Pillars of Statistical Wisdom},
	year = 2016}

@book{Wasserman2013-bc,
	abstract = {Taken literally, the title ``All of Statistics'' is an
               exaggeration. But in spirit, the title is apt, as the book does
               cover a much broader range of topics than a typical introductory
               book on mathematical statistics. This book is for people who
               want to learn probability and statistics quickly. It is suitable
               for graduate or advanced undergraduate students in computer
               science, mathematics, statistics, and related disciplines. The
               book includes modern topics like nonparametric curve estimation,
               bootstrapping, and clas sification, topics that are usually
               relegated to follow-up courses. The reader is presumed to know
               calculus and a little linear algebra. No previous knowledge of
               probability and statistics is required. Statistics, data mining,
               and machine learning are all concerned with collecting and
               analyzing data. For some time, statistics research was con
               ducted in statistics departments while data mining and machine
               learning re search was conducted in computer science
               departments. Statisticians thought that computer scientists were
               reinventing the wheel. Computer scientists thought that
               statistical theory didn't apply to their problems. Things are
               changing. Statisticians now recognize that computer scientists
               are making novel contributions while computer scientists now
               recognize the generality of statistical theory and methodology.
               Clever data mining algo rithms are more scalable than
               statisticians ever thought possible. Formal sta tistical theory
               is more pervasive than computer scientists had realized.},
	author = {Wasserman, Larry},
	date-added = {2024-04-15 20:06:49 -0700},
	date-modified = {2024-04-15 20:06:49 -0700},
	language = {en},
	month = dec,
	publisher = {Springer Science \& Business Media},
	title = {All of Statistics: A Concise Course in Statistical Inference},
	year = 2013}

@book{Tijms2012-ot,
	abstract = {Understanding Probability is a unique and stimulating approach
               to a first course in probability. The first part of the book
               demystifies probability and uses many wonderful probability
               applications from everyday life to help the reader develop a
               feel for probabilities. The second part, covering a wide range
               of topics, teaches clearly and simply the basics of probability.
               This fully revised third edition has been packed with even more
               exercises and examples and it includes new sections on Bayesian
               inference, Markov chain Monte-Carlo simulation, hitting
               probabilities in random walks and Brownian motion, and a new
               chapter on continuous-time Markov chains with applications. Here
               you will find all the material taught in an introductory
               probability course. The first part of the book, with its
               easy-going style, can be read by anybody with a reasonable
               background in high school mathematics. The second part of the
               book requires a basic course in calculus.},
	author = {Tijms, Henk},
	date-added = {2024-04-13 17:01:19 -0700},
	date-modified = {2024-04-13 17:01:19 -0700},
	language = {en},
	month = jun,
	publisher = {Cambridge University Press},
	title = {Understanding Probability},
	year = 2012}

@book{Skiena2017-ab,
	author = {Skiena, Steven S},
	date-added = {2024-04-13 15:00:20 -0700},
	date-modified = {2024-04-13 15:00:48 -0700},
	publisher = {Springer},
	title = {The Data Science Design Manual},
	year = {2017}}

@book{Wittgenstein1922-ed,
	address = {London},
	author = {Wittgenstein, Ludwig},
	date-added = {2024-04-13 11:24:21 -0700},
	date-modified = {2024-04-13 11:24:21 -0700},
	publisher = {Routledge and Kegan Paul},
	title = {Tractatus {Logico-Philosophicus}},
	year = 1922}

@book{Glymour1980-mn,
	abstract = {The Description for this book, Theory and Evidence, will be
               forthcoming.},
	author = {Glymour, Clark N},
	date-added = {2024-04-12 22:34:56 -0700},
	date-modified = {2024-04-12 22:34:56 -0700},
	language = {en},
	publisher = {Princeton University Press},
	title = {Theory and Evidence},
	year = 1980}

@article{De_Finetti1964-rr,
	abstract = {Henri Poincar{\'e}, the immortal scientist whose name this
               institute honors, and who brought to life with his ingenious
               ideas so many branches of mathematics, is without doubt also the
               thinker who attributed the greatest domain of application to the
               theory of probability and {\ldots}},
	author = {De Finetti, Bruno},
	date-added = {2024-04-12 22:30:33 -0700},
	date-modified = {2024-04-12 22:30:33 -0700},
	journal = {Studies in subjective probability},
	pages = {94--158},
	publisher = {Wiley New York},
	title = {Foresight: Its logical laws, its subjective sources},
	volume = 1964,
	year = 1964}

@book{Von_Mises1957-rk,
	address = {New York},
	author = {von Mises, Richard},
	date-added = {2024-04-12 22:26:46 -0700},
	date-modified = {2024-04-12 22:26:46 -0700},
	publisher = {Macmillan},
	title = {Probability, {S}tatistics and {T}ruth},
	year = 1957}

@book{Dale1995-zf,
	author = {Dale, Andrew I and Laplace, Pierre-Simon},
	date-added = {2024-04-12 22:24:27 -0700},
	date-modified = {2024-04-12 22:24:27 -0700},
	publisher = {Springer},
	title = {{Pierre-Simon} Laplace Philosophical Essay on Probabilities},
	year = 1995}

@book{Stalnaker1987-fw,
	abstract = {The abstract structure of inquiry - the process of acquiring and
               changing beliefs about the world - is the focus of this book
               which takes the position that the ``pragmatic'' rather than the
               ``linguistic'' approach better solves the philosophical problems
               about the nature of mental representation, and better accounts
               for the phenomena of thought and speech. It discusses
               propositions and propositional attitudes (the cluster of
               activities that constitute inquiry) in general and takes up the
               way beliefs change in response to potential new information,
               suggesting that conditional propositions should be understood as
               projections of epistemic policies onto the world.A Bradford
               Book.},
	author = {Stalnaker, Robert C},
	date-added = {2024-04-12 21:57:01 -0700},
	date-modified = {2024-04-12 21:57:01 -0700},
	language = {en},
	month = mar,
	publisher = {MIT Press},
	title = {Inquiry},
	year = 1987}

@book{Schervish2012-cn,
	abstract = {The aim of this graduate textbook is to provide a comprehensive
               advanced course in the theory of statistics covering those
               topics in estimation, testing, and large sample theory which a
               graduate student might typically need to learn as preparation
               for work on a Ph.D. An important strength of this book is that
               it provides a mathematically rigorous and even-handed account of
               both Classical and Bayesian inference in order to give readers a
               broad perspective. For example, the ``uniformly most powerful''
               approach to testing is contrasted with available
               decision-theoretic approaches.},
	author = {Schervish, Mark J},
	date-added = {2024-04-12 09:15:20 -0700},
	date-modified = {2024-04-12 09:15:25 -0700},
	language = {en},
	month = dec,
	publisher = {Springer},
	title = {Theory of Statistics},
	year = 2012}

@book{Gabbay2011-ll,
	abstract = {Statisticians and philosophers of science have many common
               interests but restricted communication with each other. This
               volume aims to remedy these shortcomings. It provides
               state-of-the-art research in the area of philosophy of
               statistics by encouraging numerous experts to communicate with
               one another without feeling ``restricted by their disciplines or
               thinking ``piecemeal in their treatment of issues. A second goal
               of this book is to present work in the field without bias toward
               any particular statistical paradigm. Broadly speaking, the
               essays in this Handbook are concerned with problems of
               induction, statistics and probability. For centuries,
               foundational problems like induction have been among
               philosophers' favorite topics; recently, however,
               non-philosophers have increasingly taken a keen interest in
               these issues. This volume accordingly contains papers by both
               philosophers and non-philosophers, including scholars from nine
               academic disciplines. Provides a bridge between philosophy and
               current scientific findings Covers theory and applications
               Encourages multi-disciplinary dialogue},
	author = {Gabbay, D M and Thagard, P and Woods, J and Bandyopadhyay, P S and {others}},
	date-added = {2024-04-12 09:15:01 -0700},
	date-modified = {2024-04-12 09:15:01 -0700},
	language = {en},
	month = may,
	publisher = {Elsevier},
	title = {Philosophy of Statistics},
	year = 2011}

@book{DeGroot2011-we,
	author = {DeGroot, Morris H and Schervish, Mark J},
	date-added = {2024-04-12 09:14:57 -0700},
	date-modified = {2024-04-12 22:47:18 -0700},
	edition = 4,
	month = jan,
	publisher = {Pearson},
	title = {Probability and Statistics},
	year = 2011}

@book{Williamson2010-dq,
	abstract = {How strongly should you believe the various propositions that
               you can express? That is the key question facing Bayesian
               epistemology. Subjective Bayesians hold that it is largely
               (though not entirely) up to the agent as to which degrees of
               belief to adopt. Objective Bayesians, on the other hand,
               maintain that appropriate degrees of belief are largely (though
               not entirely) determined by the agent's evidence. This book
               states and defends a version of objective Bayesian epistemology.
               According to this version, objective Bayesianism is
               characterized by three norms: · Probability - degrees of belief
               should be probabilities · Calibration - they should be
               calibrated with evidence · Equivocation - they should otherwise
               equivocate between basic outcomes Objective Bayesianism has been
               challenged on a number of different fronts. For example, some
               claim it is poorly motivated, or fails to handle qualitative
               evidence, or yields counter-intuitive degrees of belief after
               updating, or suffers from a failure to learn from experience. It
               has also been accused of being computationally intractable,
               susceptible to paradox, language dependent, and of not being
               objective enough. Especially suitable for graduates or
               researchers in philosophy of science, foundations of statistics
               and artificial intelligence, the book argues that these
               criticisms can be met and that objective Bayesianism is a
               promising theory with an exciting agenda for further research.},
	author = {Williamson, Jon},
	date-added = {2024-04-12 09:13:50 -0700},
	date-modified = {2024-04-12 09:13:50 -0700},
	language = {en},
	month = may,
	publisher = {Oxford University Press},
	title = {In Defence of Objective Bayesianism},
	year = 2010}

@book{Royall2017-gc,
	abstract = {Interpreting statistical data as evidence, Statistical
                 Evidence: A Likelihood Paradigm focuses on the law of
                 likelihood, fundamental to solving many of the problems
                 associated with interpreting data in this way. Statistics has
                 long neglected this principle, resulting in a seriously
                 defective methodology. This book redresses the balance,
                 explaining why science has clung to a defective methodology
                 despite its well-known defects. After examining the strengths
                 and weaknesses of the work of Neyman and Pearson and the
                 Fisher paradigm, the author proposes an alternative paradigm
                 which provides, in the law of likelihood, the explicit concept
                 of evidence missing from the other paradigms. At the same
                 time, this new paradigm retains the elements of objective
                 measurement and control of the frequency of misleading
                 results, features which made the old paradigms so important to
                 science. The likelihood paradigm leads to statistical methods
                 that have a compelling rationale and an elegant simplicity, no
                 longer forcing the reader to choose between frequentist and
                 Bayesian statistics.},
	author = {Royall, Richard},
	date-added = {2024-04-12 09:13:42 -0700},
	date-modified = {2024-04-12 09:13:42 -0700},
	language = {en},
	month = nov,
	original_id = {d69aa4c7-3667-0446-abb7-ac101969e597},
	publisher = {Routledge},
	title = {Statistical Evidence: A Likelihood Paradigm},
	year = 2017}

@incollection{Sprenger2016-xc,
	abstract = {Abstract. Bayesianism and frequentism are the two grand schools
               of statistical inference, divided by fundamentally different
               philosophical assumptions and},
	author = {Sprenger, Jan},
	booktitle = {The Oxford Handbook of Probability and Philosophy},
	date-added = {2024-04-12 09:13:31 -0700},
	date-modified = {2024-04-12 09:13:31 -0700},
	pages = {382--405},
	title = {Bayesianism vs. Frequentism in Statistical Inference},
	year = 2016}

@article{Sprenger2011-si,
	abstract = {Scientific and statistical inferences build heavily on explicit,
              parametric models, and often with good reasons. However, the
              limited scope of parametric models and the increasing complexity
              of the studied systems in modern science raise the risk of model
              misspecification. Therefore, I examine alternative, data-based
              inference techniques, such as bootstrap resampling. I argue that
              their neglect in the philosophical literature is unjustified:
              they suit some contexts of inquiry much better and use a more
              direct approach to scientific inference. Moreover, they make more
              parsimonious assumptions and often replace theoretical
              understanding and knowledge about mechanisms by careful
              experimental design. Thus, it is worthwhile to study in detail
              how nonparametric models serve as inferential engines in science.},
	author = {Sprenger, Jan},
	date-added = {2024-04-12 09:13:20 -0700},
	date-modified = {2024-04-12 09:13:20 -0700},
	journal = {Synthese},
	month = may,
	number = 1,
	pages = {65--76},
	title = {Science without (parametric) models: the case of bootstrap resampling},
	volume = 180,
	year = 2011}

@book{Sober2015-of,
	abstract = {Ockham's razor, the principle of parsimony, states that simpler
               theories are better than theories that are more complex. It has
               a history dating back to Aristotle and it plays an important
               role in current physics, biology, and psychology. The razor also
               gets used outside of science - in everyday life and in
               philosophy. This book evaluates the principle and discusses its
               many applications. Fascinating examples from different domains
               provide a rich basis for contemplating the principle's promises
               and perils. It is obvious that simpler theories are beautiful
               and easy to understand; the hard problem is to figure out why
               the simplicity of a theory should be relevant to saying what the
               world is like. In this book, the ABCs of probability theory are
               succinctly developed and put to work to describe two 'parsimony
               paradigms' within which this problem can be solved.},
	author = {Sober, Elliott},
	date-added = {2024-04-12 09:13:10 -0700},
	date-modified = {2024-04-12 09:13:10 -0700},
	language = {en},
	month = jul,
	publisher = {Cambridge University Press},
	title = {Ockham's Razors},
	year = 2015}

@book{Savage1972-zh,
	abstract = {Classic analysis of the foundations of statistics and
               development of personal probability, one of the greatest
               controversies in modern statistical thought. Revised edition.
               Calculus, probability, statistics, and Boolean algebra are
               recommended.},
	author = {Savage, Leonard J},
	date-added = {2024-04-12 09:12:58 -0700},
	date-modified = {2024-04-12 09:12:58 -0700},
	language = {en},
	publisher = {Dover},
	title = {The Foundations of Statistics},
	year = 1972}

@book{Mayo1996-re,
	abstract = {We may learn from our mistakes, but Deborah Mayo argues that,
                 where experimental knowledge is concerned, we haven't begun to
                 learn enough. Error and the Growth of Experimental Knowledge
                 launches a vigorous critique of the subjective Bayesian view
                 of statistical inference, and proposes Mayo's own
                 error-statistical approach as a more robust framework for the
                 epistemology of experiment. Mayo genuinely addresses the needs
                 of researchers who work with statistical analysis, and
                 simultaneously engages the basic philosophical problems of
                 objectivity and rationality. Mayo has long argued for an
                 account of learning from error that goes far beyond detecting
                 logical inconsistencies. In this book, she presents her
                 complete program for how we learn about the world by being
                 ``shrewd inquisitors of error, white gloves off.'' Her tough,
                 practical approach will be important to philosophers,
                 historians, and sociologists of science, and will be welcomed
                 by researchers in the physical, biological, and social
                 sciences whose work depends upon statistical analysis.},
	author = {Mayo, Deborah G},
	date-added = {2024-04-12 09:12:49 -0700},
	date-modified = {2024-04-12 09:12:49 -0700},
	language = {en},
	month = aug,
	original_id = {bd9c645b-7342-05c5-b19d-f19761ce76a1},
	publisher = {University of Chicago Press},
	title = {Error and the Growth of Experimental Knowledge},
	year = 1996}

@book{Howson2006-oy,
	abstract = {In this clearly reasoned defense of Bayes's Theorem -- that
               probability can be used to reasonably justify scientific
               theories -- Colin Howson and Peter Urbach examine the way in
               which scientists appeal to probability arguments, and
               demonstrate that the classical approach to statistical inference
               is full of flaws. Arguing the case for the Bayesian method with
               little more than basic algebra, the authors show that it avoids
               the difficulties of the classical system. The book also refutes
               the major criticisms leveled against Bayesian logic, especially
               that it is too subjective. This newly updated edition of this
               classic textbook is also suitable for college courses.},
	author = {Howson, Colin and Urbach, Peter},
	date-added = {2024-04-12 09:12:39 -0700},
	date-modified = {2024-04-12 09:12:39 -0700},
	language = {en},
	publisher = {Open Court Publishing},
	title = {Scientific Reasoning: The Bayesian Approach},
	year = 2006}

@incollection{Hitchcock2009-yj,
	author = {Hitchcock, Christopher},
	booktitle = {The Oxford Handbook of Causation},
	date-added = {2024-04-12 09:12:28 -0700},
	date-modified = {2024-04-12 09:12:28 -0700},
	pages = {299--314},
	title = {Causal Modelling},
	year = 2009}

@article{Hitchcock2001-fv,
	abstract = {WA Te live in exciting times. By'we'I mean philosophers
               study-ing the nature of causation. The past decade or so has
               witnessed a flurry of philosophical activity aimed at crack-ing
               this {\ldots}},
	author = {Hitchcock, Christopher},
	date-added = {2024-04-12 09:12:28 -0700},
	date-modified = {2024-04-12 09:12:28 -0700},
	journal = {J. Philos.},
	number = 6,
	pages = {273--299},
	publisher = {Journal of Philosophy, Inc.},
	title = {The Intransitivity of Causation Revealed in Equations and Graphs},
	volume = 98,
	year = 2001}

@article{Johnson2016-xr,
	abstract = {AbstractFamously, scientific theories are underdetermined by
               their evidence. This occurs in the factor analytic model (FA),
               which is often used to connect concrete data (e.g. test scores)
               to hypothetical notions (e.g. intelligence). After introducing
               FA, three general topics are addressed. (i) Underdetermination:
               the precise reasons why FA is underdetermined illuminates
               various claims about underdetermination, abduction, and
               theoretical terms. (ii) Uncertainties: FA helps distinguish at
               least four kinds of uncertainties. The prevailing practice,
               often encoded in statistical software, is to ignore the most
               difficult kinds, which are essential to FA's underdetermination.
               (iii) What to do: some suggestions for dealing with these
               hardest types of uncertainty are offered.},
	author = {Johnson, Kent},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	date-added = {2024-04-12 09:12:14 -0700},
	date-modified = {2024-04-12 09:12:14 -0700},
	journal = {Nous},
	language = {en},
	month = jun,
	number = 2,
	pages = {329--355},
	publisher = {Wiley},
	title = {Realism and uncertainty of unobservable common causes in factor analysis},
	volume = 50,
	year = 2016}

@book{Jeffreys1948-vt,
	author = {Jeffreys, Harold},
	date-added = {2024-04-12 09:11:58 -0700},
	date-modified = {2024-04-12 09:11:58 -0700},
	language = {en},
	publisher = {Oxford},
	title = {The Theory of Probability},
	year = 1948}

@incollection{Galavotti2014-th,
	author = {Galavotti, Maria Carla},
	booktitle = {The Routledge Companion to Philosophy of Science},
	date-added = {2024-04-12 09:11:46 -0700},
	date-modified = {2024-04-12 09:11:46 -0700},
	edition = {second},
	editor = {Curd, Martin and Psillos, Stathis},
	pages = {458--468},
	title = {Probability},
	year = 2014}

@book{Fisher1990-dp,
	author = {Fisher, Ronald A},
	date-added = {2024-04-12 09:11:34 -0700},
	date-modified = {2024-04-12 09:11:34 -0700},
	publisher = {Oxford University Press},
	title = {Statistical Methods, Experimental Design, and Scientific Inference},
	year = 1990}

@article{Efron1977-vq,
	abstract = {{\ldots} In traditional statistical theory it can be proved that no
               other estimation rule is {\ldots} The paradoxical element in Stein's re
               sult is that it sometimes contradicts this elementary law of
               statistical {\ldots}},
	author = {Efron, Bradley and Morris, Carl},
	date-added = {2024-04-12 09:10:54 -0700},
	date-modified = {2024-04-12 09:10:54 -0700},
	journal = {Sci. Am.},
	number = 5,
	pages = {119--127},
	publisher = {Scientific American, a division of Nature America, Inc.},
	title = {Stein's Paradox in Statistics},
	volume = 236,
	year = 1977}
